---
title: 数字人-基础扫盲
date: 2025-04-25 09:00:57
categories: [Digital Human]
---

## 数字人模型

### 什么是数字人模型？

“虚拟数字人模型”是近年来非常热门的一个概念，它结合了人工智能、计算机图形学、语音合成、自然语言处理等技术，用来创建和驱动一个在视觉、语音和行为上都像真人一样的“数字人”。

#### 核心构成模块
1. **视觉模型（Avatar生成）**
   - 用来生成虚拟人的三维形象（2D 或 3D）
   - 可以使用：Unity、Unreal Engine、MetaHuman、Blender 等工具
   - 技术支持：计算机图形学、人脸建模、动作捕捉

2. **语音模型（TTS & ASR）**
   - TTS：文字转语音（Text to Speech）
   - ASR：语音识别（Automatic Speech Recognition）

3. **语言理解与生成模型（NLP）**
   - 负责理解用户说的话，生成合适的回应
   - 如：GPT、ChatGLM、BERT 等大语言模型
   - 也包含意图识别、情感识别、对话管理等

4. **行为驱动模型（动画与情绪联动）**
   - 根据语音和文本生成对应的面部表情和动作
   - 技术：表情驱动（facial animation）、骨骼动画、AI驱动的姿态识别

5. **知识图谱 / 记忆系统（可选）**
   - 帮助数字人拥有长期记忆、个性化推荐能力

#### 常见技术栈
- **语言处理**：OpenAI GPT、ChatGLM、BERT、Rasa
- **语音合成/识别**：科大讯飞、腾讯云、百度语音、微软 Azure
- **3D建模**：MetaHuman、Unreal Engine、Unity
- **驱动引擎**：Unity、UE + Python/JS 脚本控制
- **集成平台**：D-ID、Synthesia、Alibaba数字人平台等


### 常用的数字人模型有哪些？

| 模型类别 | 代表模型 | 用途说明 |
|----------|----------|----------|
| 🌐 语言模型（NLP） | ChatGPT / GPT-4 / ChatGLM / Mistral / LLaMA | 处理自然语言对话，生成内容 |
| 🔊 语音合成模型（TTS） | FastSpeech、VITS、Edge TTS、科大讯飞语音 | 将文本转成语音，表达情绪语调 |
| 🎙️ 语音识别模型（ASR） | Whisper、腾讯云语音识别、百度ASR | 将用户语音转成文字，供语言模型处理 |
| 😃 表情/驱动模型 | Audio2Face、DeepMotion、Faceware | 根据语音驱动面部表情、动作 |
| 🧠 多模态模型 | GPT-4V、CLIP、Flamingo | 让数字人具备图文混合理解与生成能力 |


## LiveTalking学习

仓库地址：[LiveTalking](https://github.com/lipku/LiveTalking)

LiveTalking是一个基于数字人技术开发的实时语音交互平台，它允许用户与数字人进行实时语音交互，并实时展示数字人的表情和动作。

### 项目运行的流程

在`app.py`

在if __name__ == '__main__':块中，首先通过argparse.ArgumentParser()解析命令行参数。这些参数控制模型类型、传输协议（如WebRTC、RTMP等）、音频处理选项、TTS服务器地址等。

加载模型和头像 根据选择的模型类型（opt.model），从不同的模块（如ernerf、musetalk、wav2lip）加载相应的模型和头像。


| 模型 | 一句话功能定位 |
|------|----------------|
| **ER-NeRF** | 高保真的**3D人脸建模和渲染** |
| **MUSE-Talk** | 基于音频的**语音驱动唇形动画生成**（多情绪） |
| **Wav2Lip** | 基于音频的**视频嘴型对齐模型** |
| **Ultralight Digital Human** | 全流程数字人系统：**轻量建模 + 驱动合成** |


#### 📊 功能与特性对比表

| 项目 | **ER-NeRF** | **MUSE-Talk** | **Wav2Lip** | **Ultralight-Digital-Human** |
|-------|-------------|---------------|-------------|-------------------------------|
| 类型 | NeRF建模 | 动画生成（头部） | 嘴型对齐（视频增强） | 全栈式数字人引擎 |
| 输入 | 多视角照片 + 音频 | 音频（可选图像） | 视频 + 音频 | 语音/文本 + 图像 |
| 输出 | 高精度3D人脸 | 带嘴型动画的头像视频 | 嘴型对齐修正后视频 | 数字人口播视频 |
| 是否3D | ✅（NeRF） | ❌（伪3D/2D） | ❌ | 可2D或伪3D |
| 表情/情绪控制 | ✅ | ✅（multi-style） | ❌ | 支持一定表情合成 |
| 实时性 | ❌（较慢） | ❌（需预处理） | ✅（较快） | ✅（轻量） |
| 模型体积 | 大 | 中等 | 小 | 小（可部署） |
| 是否可商用 | ❌ 研究为主 | ❌ 研究为主 | ✅（MIT开源） | ✅（商用可谈） |
| 适用方向 | 数字人建模/高保真渲染 | 虚拟主播嘴型动画 | 提升对口型视频真实感 | 快速构建轻量数字人客服等 |

**“口型驱动模型”只是构建数字人的一小部分，而 LiveTalking 是把语音输入 → 模型推理 → 视频输出 → 播放/控制 这整条流程封装起来的完整运行框架。**

## LatentSync学习

仓库地址：[LatentSync](https://github.com/bytedance/LatentSync)

`scripts/inference.py`代码的核心解析，这是LatentSync项目的**模型推理核心实现**，实现了音视频同步生成的完整处理流程：

---

### 一、代码核心架构
| 模块 | 功能说明 | 关键技术 |
|------|----------|----------|
| **音频编码器** | 通过Whisper模型提取音频特征 | `Audio2Feature`类实现 |
| **VAE** | 视频潜在空间编码/解码 | StabilityAI的`sd-vae-ft-mse` |
| **UNet3D** | 时空特征融合与去噪 | 3D条件扩散模型 |
| **推理管线** | 协调各模块完成生成流程 | `LipsyncPipeline`类 |

### 二、关键代码流程
#### 1. 初始化阶段
```python
# 选择计算精度（根据GPU能力自动切换FP16/FP32）
is_fp16_supported = torch.cuda.get_device_capability()[0] > 7
dtype = torch.float16 if is_fp16_supported else torch.float32

# 加载扩散模型调度器
scheduler = DDIMScheduler.from_pretrained("configs")  # 使用DDIM加速采样
```

#### 2. 模型加载
```python
# 音频特征提取器（根据cross_attention_dim选择模型规模）
audio_encoder = Audio2Feature(
    model_path="checkpoints/whisper/tiny.pt",  # 384维特征
    num_frames=config.data.num_frames,         # 视频帧数（如16帧）
    audio_feat_length=config.data.audio_feat_length  # 音频特征长度
)

# 图像潜在空间编解码器
vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse")
vae.config.scaling_factor = 0.18215  # 潜在空间缩放系数

# 3D UNet去噪模型
denoising_unet = UNet3DConditionModel.from_pretrained(
    config=OmegaConf.to_container(config.model),  # 模型结构配置
    ckpt_path=args.inference_ckpt_path            # 预训练权重
).to(dtype=dtype)
```


#### 3. 推理管线构建
```python
pipeline = LipsyncPipeline(
    vae=vae,                   # 潜在空间编解码
    audio_encoder=audio_encoder,  # 音频特征提取
    denoising_unet=denoising_unet,  # 时空去噪
    scheduler=scheduler         # 扩散调度策略
).to("cuda")
```


#### 4. 生成执行
```python
pipeline(
    video_path=args.video_path,  # 输入视频（驱动姿态）
    audio_path=args.audio_path,  # 输入音频（驱动唇形）
    video_out_path=args.video_out_path,  # 输出视频路径
    num_inference_steps=args.inference_steps,  # 去噪步数（默认20）
    guidance_scale=args.guidance_scale,  # 分类器自由引导强度（默认1.0）
    width=config.data.resolution,  # 输出分辨率（如512）
    mask_image_path=config.data.mask_image_path  # 面部区域掩码
)
```


---

### 三、关键参数说明
| 参数 | 作用 | 典型值 |
|------|------|--------|
| `num_inference_steps` | 控制生成质量与速度的平衡 | 20-50步 |
| `guidance_scale` | 调节音频条件对生成的引导强度 | 1.0-3.0 |
| `num_frames` | 同时处理的视频帧数 | 16帧 |
| `resolution` | 输出视频分辨率 | 512x512 |
