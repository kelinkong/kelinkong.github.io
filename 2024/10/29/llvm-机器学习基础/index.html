<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>机器学习基础 | Kelin's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">机器学习基础</h1><a id="logo" href="/.">Kelin's blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">机器学习基础</h1><div class="post-meta">2024-10-29<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 8</span><span class="post-meta-item-text"> Minutes</span></span></span></div><div class="post-content"><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p><strong>什么是一个线性回归问题？</strong></p>
<p>如果把线性回归模型看作一个函数，用最简单的话说，这个函数的作用就是预测。</p>
<p>线性回归模型的预测过程是这样的：给定一个输入特征向量，通过线性组合得到一个预测值。</p>
<p>举个例子：</p>
<p>假设有数据集：这里我们想知道工资、年龄和贷款额度之间的关系。</p>
<table>
<thead>
<tr>
<th>工资</th>
<th>年龄</th>
<th>贷款额度</th>
</tr>
</thead>
<tbody><tr>
<td>1000</td>
<td>25</td>
<td>1000</td>
</tr>
<tr>
<td>2000</td>
<td>30</td>
<td>2000</td>
</tr>
<tr>
<td>3000</td>
<td>35</td>
<td>3000</td>
</tr>
</tbody></table>
<p>我们可以用线性回归模型来预测工资，假设我们的模型是这样的：<br>$y &#x3D; w_1 \cdot x_1 + w_2 \cdot x_2$</p>
<p>其中，$y$ 是贷款额度，$x_1$ 是工资，$x_2$ 是年龄。</p>
<p>我们的目标是找到一个合适的 $w_1$ 和 $w_2$，使得我们的模型能够很好地预测工资。</p>
<p>这个函数就是一个线性函数，也就是线性回归模型。</p>
<p>$$<br>y &#x3D; w_0 + w_1 \cdot x_1 + w_2 \cdot x_2<br>  &#x3D; \sum_{i&#x3D;0}^{n} w_i \cdot x_i<br>$$</p>
<p>这里$w_0$是偏置项，用来调整模型的预测值。$x_0$是一个常数项，通常为1。</p>
<h3 id="误差值"><a href="#误差值" class="headerlink" title="误差值"></a>误差值</h3><p>误差值是指预测值与真实值之间的差距。</p>
<p><img src="/../imgs/image-62.png"></p>
<ul>
<li>误差与误差之间是独立的，不会相互影响。</li>
<li>误差的分布是正态分布，均值为0。</li>
</ul>
<p>预测与误差：</p>
<p>$$<br>y^i &#x3D; W^T \cdot x^i + \epsilon^i<br>$$</p>
<p>其中，$y^i$ 是预测值，$W^T$ 是权重向量，$x^i$ 是输入特征向量，$\epsilon^i$ 是误差。</p>
<p>误差服从正态分布：</p>
<p>$$<br>p(\epsilon^i) &#x3D; \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(\epsilon^i)^2}{2\sigma^2})<br>$$</p>
<p>将误差代入预测值：</p>
<p>$$<br>p(y^i|x^i;W) &#x3D; \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(y^i - W^T \cdot x^i)^2}{2\sigma^2})<br>$$</p>
<p>$x^i;W$组合之后的概率密度函数，就是我们的模型。我们希望$x^i;W$组合之后与$y^i$越接近越好。所以希望最大化这个概率密度函数。</p>
<h3 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h3><p>似然函数为什么要使用连乘？因为前面说过，误差是独立的，所以我们可以将每个样本的概率密度函数连乘起来。</p>
<p>$$<br>L(W) &#x3D; \prod_{i&#x3D;1}^{m} p(y^i|x^i;W)&#x3D; \prod_{i&#x3D;1}^{m} \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(y^i - W^T \cdot x^i)^2}{2\sigma^2})<br>$$</p>
<p>对似然函数取对数，可以将连乘转换为连加：</p>
<p>$$<br>\log L(W) &#x3D; \sum_{i&#x3D;1}^{m} \log p(y^i|x^i;W)&#x3D; \sum_{i&#x3D;1}^{m} \log \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(y^i - W^T \cdot x^i)^2}{2\sigma^2})<br>$$</p>
<p>展开化简：</p>
<p>$$<br>\log L(W) &#x3D; \frac{m}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i&#x3D;1}^{m} (y^i - W^T \cdot x^i)^2<br>$$</p>
<p>我们最终的目标是最大化似然函数，也就是最小化误差。前一项是常数，所以希望后一项越小越好。即：</p>
<p>$$<br>\min_W \frac{1}{2} \sum_{i&#x3D;1}^{m} (y^i - W^T \cdot x^i)^2<br>$$</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>什么是损失函数？</strong></p>
<p>损失函数也叫目标函数，是用来衡量模型预测值与真实值之间的差距的函数。</p>
<p>上面那个例子，我们的目标是找到一个合适的 $w_1$ 和 $w_2$，使得我们的模型能够很好地预测工资。损失函数可以定义为：</p>
<p>$$<br>L(w) &#x3D; \frac{1}{2m} \sum_{i&#x3D;1}^{m} (y_i - \hat{y_i})^2<br>$$</p>
<p>使用目标值和预测值之间的平方误差作为损失函数，这个损失函数叫做均方误差（Mean Squared Error，MSE）。</p>
<p>其中，$m$ 是样本数量，$y_i$ 是真实值，$\hat{y_i}$ 是预测值。</p>
<p>对于机器学习的大部分任务，我们都是通过最小化损失函数来优化模型的参数。</p>
<p>如何求解损失函数？</p>
<p>$$<br>L(w) &#x3D; \frac{1}{2m} \sum_{i&#x3D;1}^{m} (W^T \cdot x^i - \hat{y_i})^2 &#x3D; \frac{1}{2m} (X \cdot W - Y)^T \cdot (X \cdot W - Y)<br>$$</p>
<p>其中，$X$ 是输入特征矩阵，$Y$ 是真实值矩阵。</p>
<p>我们的目标是找到一个合适的 $W$，使得损失函数最小化。</p>
<p>因此，对损失函数求导，然后令导数为0，可以得到最优解。</p>
<p>$$<br>\frac{\partial L(W)}{\partial W} &#x3D; \frac{1}{m} X^T \cdot (X \cdot W - Y) &#x3D; 0<br>$$</p>
<p>解方程，得到最优解：<br>$$<br>W &#x3D; (X^T \cdot X)^{-1} \cdot X^T \cdot Y<br>$$</p>
<p>但是，这个方法有一个问题，就是计算量太大。当数据量很大时，计算矩阵的逆是非常耗时的。而且，矩阵的逆不一定存在。</p>
<p>因此，我们通常使用梯度下降法来求解损失函数的最小值。</p>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p><strong>什么是梯度下降法？</strong></p>
<p>梯度下降法是一种常用的优化算法，用来求解损失函数的最小值。</p>
<p>梯度下降法的思想是：沿着梯度的反方向，不断迭代更新参数，直到损失函数的值收敛。</p>
<p><strong>目标函数</strong></p>
<p>$$<br>J(\theta_0, \theta_1) &#x3D; \frac{1}{2m} \sum_{i&#x3D;1}^{m} (h_\theta(x^i) - y^i)^2<br>$$</p>
<p>其中，$h_\theta(x^i) &#x3D; \theta_0 + \theta_1 \cdot x^i$ 是模型的预测值。</p>
<p><strong>如何寻找合适的方向？</strong></p>
<p>梯度下降法的核心是求解目标函数的梯度。</p>
<p>分别对 $\theta_0$ 和 $\theta_1$ 求偏导数：</p>
<p>$$<br>\frac{\partial J(\theta_0, \theta_1)}{\partial \theta_1} &#x3D; \frac{1}{m} \sum_{i&#x3D;1}^{m} (y^i - h_\theta(x^i))x^i_1<br>$$</p>
<p>使得梯度为0，得到最优解。因为切线方向是函数值下降最快的方向，此时梯度为0。梯度其实就是函数的导数。</p>
<p>注意，这里等式中的两个$\theta_j$表示两个不同的$\theta_j$，一个是当前的$\theta_j$，一个是更新后的$\theta_j$。</p>
<p>$$<br>\theta_j &#x3D;  \theta_j - \alpha  \frac{\partial J(\theta_j)}{\partial \theta_j}&#x3D;<br> \theta_j - \alpha \frac{1}{m} \sum_{i&#x3D;1}^{m} (y^i - h_\theta(x^i))x^i_j<br>$$</p>
<p>其中，$\alpha$ 是学习率，用来控制参数更新的步长，$x^i_j$表示第$i$个样本的第$j$个特征。</p>
<p>如果将所有的参数更新写成矩阵形式，可以得到：</p>
<p>$$<br>\theta &#x3D; \theta - \alpha \frac{1}{m} X^T \cdot (X \cdot \theta - Y)<br>$$</p>
<p>这就是梯度下降法中的批量梯度下降（Batch Gradient Descent）。但是，批量梯度下降的计算量很大，因为每次迭代都要计算所有样本的梯度。</p>
<h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>随机梯度下降（Stochastic Gradient Descent，SGD）是梯度下降法的一种变种，它每次迭代只使用一个样本来更新参数。</p>
<p>$$<br>\theta &#x3D; \theta - \alpha (h_\theta(x^i) - y^i) x^i<br>$$</p>
<p>其中，$x^i$ 是第$i$个样本的特征向量，$y^i$ 是第$i$个样本的真实值。</p>
<p>随机梯度下降的优点是计算速度快，但缺点是收敛速度慢，因为每次迭代的方向不一定是最优的。</p>
<h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>小批量梯度下降（Mini-batch Gradient Descent）是批量梯度下降和随机梯度下降的折中方案，每次迭代使用一小部分样本来更新参数。</p>
<p>$$<br>\theta &#x3D; \theta - \alpha \frac{1}{m} \sum_{i&#x3D;1}^{m} (h_\theta(x^i) - y^i) x^i<br>$$</p>
<p>其中，$m$ 是小批量的大小。batch size的选择对模型的训练速度有很大影响。当batch size较小时，模型的训练速度较慢；当batch size较大时，模型的训练速度较快。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>给一个代码实现：</p>
<p>对应上面的公式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, learning_rate=<span class="number">0.01</span>, num_iterations=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用梯度下降法训练线性回归模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    cost_history = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">        self.theta = self.predict_step(learning_rate)</span><br><span class="line">        cost_history.append(self.compute_cost())</span><br><span class="line">        logger.info(<span class="string">&quot;第&#123;&#125;次迭代，损失值为&#123;&#125;&quot;</span>.<span class="built_in">format</span>(_, cost_history[-<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回训练后的参数和损失值</span></span><br><span class="line">    <span class="keyword">return</span> self.theta, cost_history</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_step</span>(<span class="params">self, learning_rate</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    每一步迭代计算梯度并更新参数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_examples = self.data.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 计算预测值</span></span><br><span class="line">    prediction = self.hypothesis(self.data, self.theta)</span><br><span class="line">    <span class="comment"># 计算误差</span></span><br><span class="line">    error = prediction - self.labels</span><br><span class="line">    <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">    theta = self.theta - learning_rate * (<span class="number">1</span> / num_examples) * np.dot(self.data.T, error)</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hypothesis</span>(<span class="params">data, theta</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算预测值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(data, theta)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算损失, 这里使用均方误差</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_examples = self.data.shape[<span class="number">0</span>]</span><br><span class="line">    prediction = self.hypothesis(self.data, self.theta)</span><br><span class="line">    error = prediction - self.labels</span><br><span class="line">    cost = (<span class="number">1</span> / (<span class="number">2</span> * num_examples)) * np.dot(error.T, error)</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure></div><div class="tags"></div><div class="post-nav"><a class="pre" href="/2024/11/01/llvm-%E5%BE%AE%E8%B0%83%E5%A4%A7%E6%A8%A1%E5%9E%8B/">llvm-微调大模型</a><a class="next" href="/2024/10/23/Spring-Boot%EF%BC%9ASpring-MVC/">Spring Boot：Spring MVC</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://kelinkong.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img class="nofancybox" src="/img/logo.jpg"/></a><p>Kelin is a boring girl, but not always.</p><a class="info-icon" href="https://github.com/kelinkong" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CPP/">CPP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Frontend/">Frontend</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LLVM/">LLVM</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Network-Security/">Network Security</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Operating-System/">Operating System</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Solutions/">Solutions</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/CMU15-445/" style="font-size: 15px;">CMU15-445</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/note/" style="font-size: 15px;">note</a> <a href="/tags/architecture/" style="font-size: 15px;">architecture</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/11/22/%E5%89%8D%E7%AB%AF-expo/">前端-expo</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/19/Spring-Cloud-%E6%96%AD%E8%B7%AF%E5%99%A8/">Spring Cloud: 断路器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/14/Spring-Cloud-OpenFeign/">Spring Cloud: OpenFeign</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/14/Spring-Cloud-%E6%96%B0%E5%A2%9E%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%9D%97/">Spring Cloud:服务注册与发现</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/13/Spring-Cloud%EF%BC%9Abase%E5%B7%A5%E7%A8%8B/">Spring Cloud：base工程</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/13/Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/">Java学习笔记-配置文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/lvvm-%E4%BD%BF%E7%94%A8langchain-chatchat%E5%92%8Collama%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B/">llvm-使用langchain-chatchat和ollama构建大模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/llvm-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E5%85%B3%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B7%A5%E5%85%B7%E9%93%BE/">大模型有关的一些工具链</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/06/llvm-%E4%BD%BF%E7%94%A8LCEL-%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84-LLM-%E5%BA%94%E7%94%A8/">使用 LCEL 构建一个简单的 LLM 应用</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/01/llvm-%E5%BE%AE%E8%B0%83%E5%A4%A7%E6%A8%A1%E5%9E%8B/">llvm-微调大模型</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Kelin's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/love.js?v=1.0.0"></script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>