<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>llvm学习-人工神经网络 | Kelin's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">llvm学习-人工神经网络</h1><a id="logo" href="/.">Kelin's blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">llvm学习-人工神经网络</h1><div class="post-meta">2024-10-18<span> | </span><span class="category"><a href="/categories/LLVM/">LLVM</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.6k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 12</span><span class="post-meta-item-text"> Minutes</span></span></span></div><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在学习大模型相关的知识，很多教程都会提到人工神经网络。读研时总是听到训练神经网络之类的词，那这个训练，到底是训练什么呢？</p>
<p>我们现在假设要训练的模型是一个函数，首先考虑，这个函数的作用是什么？这个函数的输入是什么？输出是什么？</p>
<p>人工神经网络可以用来做分类、预测、生成等任务，现在假设我们的函数是一个分类函数，那么输入就是一个数据，输出就是这个数据属于哪个类别。</p>
<p>所以，训练神经网络，就是训练这个函数，让这个函数能够准确地对数据进行分类。</p>
<p>所谓训练，有点太拟人化了，听起来比较抽象，其实就是调整函数的参数，使得函数的输出尽可能接近真实值。</p>
<p>参数又是什么？这里先介绍一下神经网络的基本结构。</p>
<h2 id="人工神经网络的基本结构"><a href="#人工神经网络的基本结构" class="headerlink" title="人工神经网络的基本结构"></a>人工神经网络的基本结构</h2><p>人工神经网络是一种模拟人脑神经元网络的计算模型，它由多个神经元（Neuron）组成，每个神经元接收多个输入，经过加权和激活函数处理后，输出一个值。</p>
<h3 id="神经元（Neuron）"><a href="#神经元（Neuron）" class="headerlink" title="神经元（Neuron）"></a>神经元（Neuron）</h3><p><img src="/../imgs/image-51.png" alt="图来源于维基百科"></p>
<p>一个神经元的结构可以分为三个主要部分：<strong>输入</strong>、<strong>处理单元</strong>、<strong>输出</strong>。如果用前文的函数来描述，每一个神经元就是一个函数，这个函数的输入就是一个特征向量和一个权重向量，这个向量经过某种计算后，输出一个值（一个神经元的输出通常是一个标量）。</p>
<p>经过了什么处理呢？</p>
<p>对于单个神经元，其数学表达可以写为：<br>   $$<br>   y &#x3D; \text{activation}(w_1 \cdot x_1 + w_2 \cdot x_2 + \dots + w_n \cdot x_n + b)<br>   $$</p>
<p>具体来看：</p>
<h4 id="1-输入层（Input-Layer）："><a href="#1-输入层（Input-Layer）：" class="headerlink" title="1. 输入层（Input Layer）："></a>1. <strong>输入层（Input Layer）</strong>：</h4><ul>
<li>神经元的输入来自前一层的神经元，或者在网络的第一层，输入是来自外部的数据。</li>
<li>输入信号（即特征）通过**权重(Weights)**传递到神经元。每个输入信号都有一个权重值，权重表示输入的重要性。</li>
</ul>
<p>   <strong>示例</strong>：<br>   如果我们有三个输入特征 $( a_1, a_2, a_3 )$，每个输入都有一个对应的权重 $( w_1, w_2, w_3 )$。<br>   $$<br>   z &#x3D; w_1 \cdot a_1 + w_2 \cdot a_2 + w_3 \cdot a_3 + b<br>   $$<br>   其中，$b$ 是偏置项。</p>
<h4 id="2-加权求和（Weighted-Sum）："><a href="#2-加权求和（Weighted-Sum）：" class="headerlink" title="2. 加权求和（Weighted Sum）："></a>2. <strong>加权求和（Weighted Sum）</strong>：</h4><ul>
<li>输入通过权重后，神经元会将这些加权输入进行加和，再加上一个<strong>偏置（Bias）</strong>项。</li>
<li>偏置用于调整激活函数的输出，让神经元的输出更加灵活，帮助网络更好地拟合数据。</li>
</ul>
<p>   <strong>数学表示</strong>：<br>   $$<br>   z &#x3D; \sum (w_i \cdot x_i) + b<br>   $$</p>
<p>   其中，$z$ 是加权求和的结果，$w_i$ 是权重，$x_i$ 是输入，$b$是偏置。</p>
<h4 id="3-激活函数（Activation-Function）："><a href="#3-激活函数（Activation-Function）：" class="headerlink" title="3. 激活函数（Activation Function）："></a>3. <strong>激活函数（Activation Function）</strong>：</h4><ul>
<li>加权求和后的结果$z$ 会通过一个<strong>激活函数</strong>，以增加神经元的非线性能力。</li>
<li>激活函数的目的是引入非线性，以便神经网络能够学习复杂的模式。常见的激活函数有 Sigmoid、ReLU、Tanh 等。</li>
</ul>
<h4 id="4-输出（Output）："><a href="#4-输出（Output）：" class="headerlink" title="4. 输出（Output）："></a>4. <strong>输出（Output）</strong>：</h4><ul>
<li>经过激活函数处理后的输出 $t$ 是神经元的最终结果，这个结果可以作为下一层神经元的输入，或者作为最终输出（例如分类结果或回归值）。</li>
<li>在输出层，输出值可以是一个分类标签（如Softmax输出的类别）或一个回归值（如预测的数值）。</li>
</ul>
<p>我们现在已经知道单个神经元（函数）做了什么工作，那么，为什么要经过加权求和、偏置又是什么、为什么要有激活函数？</p>
<h3 id="加权求和和偏置"><a href="#加权求和和偏置" class="headerlink" title="加权求和和偏置"></a>加权求和和偏置</h3><h4 id="1-加权求和："><a href="#1-加权求和：" class="headerlink" title="1. 加权求和："></a>1. <strong>加权求和</strong>：</h4><ul>
<li>加权求和是为了给不同的输入信号赋予不同的重要性。</li>
<li>通过调整权重，神经元可以学习到不同特征的重要性，从而更好地拟合数据。</li>
</ul>
<p>比如说，判断一个人是男人还是女人，我们可以用身高、体重、声音等特征。但是，不同的特征对性别的判断重要性是不同的，比如身高可能比声音更重要，所以我们需要通过权重来调整这些特征的重要性。</p>
<h4 id="2-偏置："><a href="#2-偏置：" class="headerlink" title="2. 偏置："></a>2. <strong>偏置</strong>：</h4><p>偏置的作用是让神经网络的输出有更多的灵活性，即使输入的值为零，偏置也能确保神经元的激活函数能产生非零的输出。这有助于模型更好地拟合数据。</p>
<p>为什么要有偏置：如果没有偏置，神经网络的输出完全依赖于输入数据。如果输入为零，输出会总是零，这样模型的表达能力受到限制。通过加入偏置，神经网络可以更好地学习复杂的模式和关系。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>目的：激活函数的主要作用是引入非线性，从而让神经网络能够处理复杂的非线性问题。</p>
<p>没有激活函数，神经网络只会进行线性变换，加权求和的输出只是输入的线性组合。线性模型的能力有限，无法拟合复杂的非线性数据。因此，激活函数通过对加权和进行非线性处理，使网络可以处理高度复杂的任务，如图像识别、自然语言处理等。</p>
<h4 id="1-ReLU（Rectified-Linear-Unit）："><a href="#1-ReLU（Rectified-Linear-Unit）：" class="headerlink" title="1. ReLU（Rectified Linear Unit）："></a>1. <strong>ReLU（Rectified Linear Unit）</strong>：</h4><p>   ReLU 是最简单的激活函数之一。它的数学表达式为：<br>   $$<br>   \text{ReLU}(z) &#x3D; \max(0, z)<br>   $$</p>
<ul>
<li>当 $z &gt; 0$，输出 $z$。</li>
<li>当 $z \leq 0$，输出 0。</li>
</ul>
<h4 id="2-Sigmoid："><a href="#2-Sigmoid：" class="headerlink" title="2. Sigmoid："></a>2. <strong>Sigmoid</strong>：</h4><p>   Sigmoid 是一种常用的激活函数，尤其适用于二分类任务。它的数学表达式为：<br>   $$<br>   \sigma(z) &#x3D; \frac{1}{1 + e^{-z}}<br>   $$</p>
<ul>
<li>该函数将输入 $z$ 映射到区间 (0, 1) 之间。</li>
<li>当 $z \to \infty$，Sigmoid 函数的输出接近 1。</li>
<li>当 $z \to -\infty$，输出接近 0。</li>
</ul>
<h4 id="3-Tanh（Hyperbolic-Tangent）："><a href="#3-Tanh（Hyperbolic-Tangent）：" class="headerlink" title="3. Tanh（Hyperbolic Tangent）："></a>3. <strong>Tanh（Hyperbolic Tangent）</strong>：</h4><p>   Tanh 是另一个常用的激活函数，它将输入映射到 -1 到 1 之间。其数学表达式为：<br>   $$<br>   \tanh(z) &#x3D; \frac{e^z - e^{-z}}{e^z + e^{-z}}<br>   $$</p>
<ul>
<li>该函数将 $z$ 映射到区间 (-1, 1)。</li>
<li>当 $z \to \infty$，Tanh 的输出接近 1。</li>
<li>当 $z \to -\infty$，输出接近 -1。</li>
</ul>
<h4 id="这三个激活函数的行为："><a href="#这三个激活函数的行为：" class="headerlink" title="这三个激活函数的行为："></a>这三个激活函数的行为：</h4><ul>
<li><strong>ReLU</strong>：输出非负的值，有助于解决梯度消失问题，但可能会导致“死亡ReLU”问题（即神经元在某些区域永远输出0）。</li>
<li><strong>Sigmoid</strong>：适用于二分类，但在极值区域梯度容易消失。</li>
<li><strong>Tanh</strong>：输出范围为 (-1, 1)，通常比 Sigmoid 更有效，但仍可能存在梯度消失问题。</li>
</ul>
<p>偏置让模型产生非零输出，激活函数引入非线性，这两个因素使得神经网络能够学习复杂的模式和关系。那什么是线性模型，什么是非线性模型呢？</p>
<h3 id="线性模型和非线性模型"><a href="#线性模型和非线性模型" class="headerlink" title="线性模型和非线性模型"></a>线性模型和非线性模型</h3><p>我们先从“线性模型”的概念开始，然后再解释为什么它的能力有限，以及它为什么不能拟合复杂的非线性数据。</p>
<h4 id="1-线性模型是什么？"><a href="#1-线性模型是什么？" class="headerlink" title="1. 线性模型是什么？"></a>1. <strong>线性模型是什么？</strong></h4><ul>
<li>线性模型是一种简单的数学模型，它通过输入变量（特征）的线性组合来进行预测。</li>
<li>公式形式为：<br>$$<br>y &#x3D; w_1 \cdot x_1 + w_2 \cdot x_2 + \dots + w_n \cdot x_n + b<br>$$<br>其中：<ul>
<li>$y$ 是模型的输出（预测结果），</li>
<li>$x_1, x_2, …, x_n$ 是输入特征，</li>
<li>$w_1, w_2, …, w_n$ 是每个特征的权重，</li>
<li>$b$ 是偏置项。</li>
</ul>
</li>
</ul>
<p>   这个公式表明输出 $y$ 是输入特征的<strong>加权求和</strong>。这种模型在数据的所有关系都是线性时，表现得很好。</p>
<h4 id="2-线性模型的局限性"><a href="#2-线性模型的局限性" class="headerlink" title="2. 线性模型的局限性"></a>2. <strong>线性模型的局限性</strong></h4><ul>
<li>线性模型可以很好地处理简单的任务，比如预测身高和体重的关系，如果这两个变量之间的关系是线性的。</li>
<li><strong>线性关系</strong>意味着，输入特征的变化会导致输出以固定比例变化（即没有复杂的交互或非线性效应）。</li>
</ul>
<p>   例如，如果我们在二维平面上绘制一条直线，直线可以通过简单的线性方程来表示，适用于处理两个变量之间的简单比例关系。但在现实世界中，很多问题不是这样简单的线性关系。</p>
<h4 id="3-什么是非线性数据？"><a href="#3-什么是非线性数据？" class="headerlink" title="3. 什么是非线性数据？"></a>3. <strong>什么是非线性数据？</strong></h4><ul>
<li>非线性数据是指输入与输出之间的关系无法用简单的直线表示。它可能是弯曲的、复杂的、有多个转折点，或者数据之间存在高度的交互。</li>
<li>比如，如果想预测一个人的年收入，可能会涉及教育背景、工作经验、职位、技能等多个复杂因素，它们之间的关系是高度复杂的，这样的数据就是<strong>非线性的</strong>。</li>
</ul>
<p>   例如，如果我们用简单的线性模型来处理这个问题，它只能画出一条直线，但现实中这些因素的关系是复杂而非线性的，可能是一个波动的曲线。线性模型无法正确表达这种复杂的关系。</p>
<h4 id="4-为什么线性模型不能拟合复杂的非线性数据？"><a href="#4-为什么线性模型不能拟合复杂的非线性数据？" class="headerlink" title="4. 为什么线性模型不能拟合复杂的非线性数据？"></a>4. <strong>为什么线性模型不能拟合复杂的非线性数据？</strong></h4><ul>
<li>线性模型的核心在于它只会根据输入特征进行简单的加权和。因此，它无法捕捉到变量之间复杂的交互关系或弯曲的趋势。</li>
<li>举个简单的例子，假设要预测物体在抛出后的运动轨迹。物体的轨迹是抛物线（非线性），如果你用线性模型去拟合这个轨迹，结果只能是一条直线，完全无法准确描述抛物线的运动。</li>
<li>现实中的大多数数据，比如图像、语音、自然语言等，往往存在复杂的非线性关系，输入特征之间的交互也很复杂，线性模型无法有效捕捉这些关系。</li>
</ul>
<h4 id="5-如何解决这个问题？"><a href="#5-如何解决这个问题？" class="headerlink" title="5. 如何解决这个问题？"></a>5. <strong>如何解决这个问题？</strong></h4><ul>
<li>为了解决这个问题，我们需要引入<strong>非线性模型</strong>。神经网络就是一种非常强大的非线性模型，它通过<strong>激活函数</strong>（如 ReLU、Sigmoid）引入非线性，使得神经网络能够处理和拟合复杂的非线性数据。</li>
<li>每一层神经元通过权重和偏置计算出加权和，并通过激活函数引入非线性，逐层递进地学习输入数据的复杂模式，从而对复杂任务做出准确预测。</li>
</ul>
<p>用大白话说就是线性模型是拟合为直线的，非线性模型可以拟合为曲线的。</p>
<p><img src="/../imgs/image-52.png"></p>
<p><strong>那么，为什么加入激活函数后，神经网络就能处理非线性数据了呢？</strong></p>
<blockquote>
<p>当我们给神经网络加入激活函数后，它就能够处理复杂的非线性数据，原因很简单：</p>
<p>打破线性限制：如果没有激活函数，神经网络每一层的计算都是线性的（像画直线一样），无论堆叠多少层，最终的输出也只是线性组合，无法处理弯曲、复杂的模式。</p>
<p>激活函数引入变化：激活函数就像给网络“加了个弯”，不再只产生直线关系。比如 ReLU 会把负数变成 0，而正数保持不变，这种变化让网络能处理更复杂的情况，不再是简单的线性相加。<br>层层递进：每一层加上激活函数后，输出的结果会经过更多复杂的非线性处理。最终，网络能学会从数据中找到隐藏的复杂关系，甚至是一些人类看不到的特征。<br>简单来说，激活函数让网络能够画“曲线”，而不是只能画“直线”，这就是它能处理非线性数据的原因。</p>
</blockquote>
<p>前面提到：输出非负的值，有助于解决梯度消失问题。</p>
<p><strong>那么，梯度是什么？梯度消失又是什么？</strong></p>
<h3 id="梯度和梯度消失"><a href="#梯度和梯度消失" class="headerlink" title="梯度和梯度消失"></a>梯度和梯度消失</h3><p>我们先从<strong>梯度</strong>的概念开始，接着解释什么是<strong>梯度消失</strong>。</p>
<h4 id="1-什么是梯度？"><a href="#1-什么是梯度？" class="headerlink" title="1. 什么是梯度？"></a>1. <strong>什么是梯度？</strong></h4><ul>
<li>在神经网络中，梯度指的是损失函数（模型预测的误差）对模型参数（权重和偏置）的导数。</li>
<li>梯度告诉我们如何调整权重才能让损失减少，也就是说，它指示了优化方向。</li>
<li>比如，假设我们在爬山，梯度就像告诉我们当前所处位置的坡度和方向，帮助我们找到“山顶”（即最小的损失值）。</li>
</ul>
<h4 id="2-梯度消失是什么？"><a href="#2-梯度消失是什么？" class="headerlink" title="2. 梯度消失是什么？"></a>2. <strong>梯度消失是什么？</strong></h4><ul>
<li><strong>梯度消失</strong>是指当神经网络的某些层的梯度变得非常小时，网络更新参数的速度变得非常慢，甚至无法继续学习。</li>
<li>在训练深层神经网络时，随着误差从输出层逐层向前传播回输入层，梯度会逐渐变小。尤其是当使用像 Sigmoid 或 Tanh 这样的激活函数时，它们的导数很容易变得接近 0，导致梯度消失。</li>
</ul>
<h4 id="3-为什么梯度消失是个问题？"><a href="#3-为什么梯度消失是个问题？" class="headerlink" title="3. 为什么梯度消失是个问题？"></a>3. <strong>为什么梯度消失是个问题？</strong></h4><ul>
<li>梯度消失意味着，神经网络前面几层的权重几乎没有更新。这会导致这些层的学习速度非常慢，网络无法有效学习数据中的复杂特征。</li>
<li>如果梯度消失，模型在训练过程中就会变得“卡住”，无法通过反向传播有效优化权重，这样模型的表现也就无法提高。</li>
</ul>
<h4 id="4-为什么输出非负值可以帮助解决梯度消失？"><a href="#4-为什么输出非负值可以帮助解决梯度消失？" class="headerlink" title="4. 为什么输出非负值可以帮助解决梯度消失？"></a>4. <strong>为什么输出非负值可以帮助解决梯度消失？</strong></h4><ul>
<li><strong>ReLU激活函数</strong>输出非负值（即当输入大于 0 时，输出等于输入；否则输出 0）。ReLU 的导数是常数 1（对于正数部分），不会像 Sigmoid 或 Tanh 那样迅速衰减为接近 0。这意味着通过 ReLU，网络的梯度不容易消失，前面几层的学习速度也能得到保证。</li>
<li>换句话说，<strong>输出非负的值</strong>，特别是通过 ReLU 这样的激活函数，可以避免导数过小，从而缓解梯度消失问题。</li>
</ul>
</div><div class="tags"></div><div class="post-nav"><a class="pre" href="/2024/10/22/Spring-Boot%EF%BC%9A%E5%85%A8%E6%B3%A8%E8%A7%A3%E4%B8%8B%E7%9A%84loc/">Spring Boot：全注解下的IoC</a><a class="next" href="/2024/10/12/llvm-RAG%E5%AD%A6%E4%B9%A0/">llvm-RAG学习</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://kelinkong.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img class="nofancybox" src="/img/logo.jpg"/></a><p>Kelin is a boring girl, but not always.</p><a class="info-icon" href="https://github.com/kelinkong" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CPP/">CPP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Frontend/">Frontend</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LLVM/">LLVM</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Network-Security/">Network Security</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Operating-System/">Operating System</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Solutions/">Solutions</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/frontend/">frontend</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/llvm/">llvm</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/CMU15-445/" style="font-size: 15px;">CMU15-445</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/note/" style="font-size: 15px;">note</a> <a href="/tags/architecture/" style="font-size: 15px;">architecture</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E5%85%B3%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B7%A5%E5%85%B7%E9%93%BE/">大模型有关的一些工具链</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/06/%E4%BD%BF%E7%94%A8-LCEL-%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84-LLM-%E5%BA%94%E7%94%A8/">使用 LCEL 构建一个简单的 LLM 应用</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/01/llvm-%E5%BE%AE%E8%B0%83%E5%A4%A7%E6%A8%A1%E5%9E%8B/">llvm-微调大模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/10/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">机器学习基础</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/10/23/Spring-Boot%EF%BC%9ASpring-MVC/">Spring Boot：Spring MVC</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/10/22/Spring-Boot%EF%BC%9A%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/">Spring Boot：事务处理</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/10/22/Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-MyBatis/">Java学习笔记-MyBatis</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/10/22/Spring-Boot%EF%BC%9A%E5%85%A8%E6%B3%A8%E8%A7%A3%E4%B8%8B%E7%9A%84loc/">Spring Boot：全注解下的IoC</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/10/18/llvm-%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">llvm学习-人工神经网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/10/12/llvm-RAG%E5%AD%A6%E4%B9%A0/">llvm-RAG学习</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Kelin's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/love.js?v=1.0.0"></script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html><!-- 插入 Dify 大模型助手代码--><script>window.difyChatbotConfig = {
  token: '4gcHypq3JpvHcBxt'
}</script><script src="https://udify.app/embed.min.js" id="4gcHypq3JpvHcBxt" defer></script><style>#dify-chatbot-bubble-button {
  background-color: #1C64F2 !important;
  position: fixed !important;
  bottom: 20px !important;
  left: 20px !important;
}
#dify-chatbot-bubble-window {
  width: 24rem !important;
  height: 40rem !important;
  bottom: 70px !important; /* 确保窗口在按钮上方 */
  left: 20px !important;
}</style>