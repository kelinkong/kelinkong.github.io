<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>AI-动手学深度学习-笔记 | Kelin's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">AI-动手学深度学习-笔记</h1><a id="logo" href="/.">Kelin's blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">AI-动手学深度学习-笔记</h1><div class="post-meta">2024-12-27<span> | </span><span class="category"><a href="/categories/AI/">AI</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 5.7k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 20</span><span class="post-meta-item-text"> Minutes</span></span></span></div><div class="post-content"><h2 id="张量">张量</h2>
<p>当张量的形状为 <code>(3, 4, 4)</code> 时，可以将其理解为一个 <strong>包含多个层的神经网络模型</strong>。让我们用 <strong>特征和权重</strong> 来比喻：</p>
<h3 id="比喻解释">比喻解释</h3>
<p>假设你有一个神经网络模型，输入层有 4 个特征（即 4 个特征值），并且你有 4 个权重值用于每个特征的计算。</p>
<ul>
<li><strong>3</strong>：表示神经网络中有 <strong>3 个不同的层</strong>（例如 3 个不同的神经网络中的权重矩阵）。</li>
<li><strong>4</strong>：每个层的 <strong>输入特征有 4 个</strong>（例如每一层有 4 个特征或神经元）。</li>
<li><strong>4</strong>：每个层的 <strong>每个特征有 4 个权重值</strong>（即每个特征都被 4 个不同的权重处理，或者每个神经元的连接数是 4）。</li>
</ul>
<h3 id="举个例子">举个例子</h3>
<p>假设你有一个多层感知机（MLP）神经网络模型，其中每一层的输入和输出都是 4 维的向量，而每一层的每个神经元的权重是一个 4 维的向量。张量的形状 <code>(3, 4, 4)</code> 就可以表示：</p>
<ul>
<li>第一维 <code>3</code> 代表神经网络中有 3 层（或 3 个不同的权重矩阵）。</li>
<li>第二维和第三维 <code>4</code> 代表每个矩阵（层）的大小是 4x4，这意味着每一层有 4 个输入特征，并且每个特征有 4 个对应的权重。</li>
</ul>
<h3 id="具体举例">具体举例</h3>
<p>假设你有以下模型：</p>
<ol>
<li><strong>层 1</strong>：输入是 4 个特征，输出是 4 个神经元。每个神经元有 4 个权重，意味着每个神经元的计算有 4 个权重与输入特征进行相乘。</li>
<li><strong>层 2</strong>：同样，输入是 4 个特征，输出也是 4 个神经元，每个神经元有 4 个权重。</li>
<li><strong>层 3</strong>：依然是 4 个输入特征，4 个神经元和 4 个权重。</li>
</ol>
<p>这样，总共有 3 个层，每层都有一个 4x4 的权重矩阵，表示每个特征在该层中如何与其他特征结合形成输出。</p>
<h3 id="代码示例">代码示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设这是一个 3 层的神经网络，每层有 4 个输入特征，4 个权重</span></span><br><span class="line">weights = torch.tensor([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>], [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>]],</span><br><span class="line">                       [[<span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>], [<span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>], [<span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>], [<span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>]],</span><br><span class="line">                       [[<span class="number">33</span>, <span class="number">34</span>, <span class="number">35</span>, <span class="number">36</span>], [<span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">40</span>], [<span class="number">41</span>, <span class="number">42</span>, <span class="number">43</span>, <span class="number">44</span>], [<span class="number">45</span>, <span class="number">46</span>, <span class="number">47</span>, <span class="number">48</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(weights.shape)  <span class="comment"># 输出: torch.Size([3, 4, 4])</span></span><br></pre></td></tr></table></figure>
<h3 id="解释">解释</h3>
<ul>
<li>第一维 <code>3</code> 表示你有 <strong>3 个层</strong>，每层有一个 4x4 的权重矩阵。</li>
<li>第二维和第三维的 <code>4</code> 表示每个层的每个神经元的计算中使用了 4 个权重，每个权重都与输入特征相乘。</li>
</ul>
<h2 id="正则化">正则化</h2>
<p>正则化（Regularization）是机器学习中用来<strong>防止模型过拟合</strong>的一种技术。过拟合是指模型在训练数据上表现很好，但在新数据（测试数据）上表现差，通常是因为模型太复杂，学习到了数据中的噪音或不重要的细节。</p>
<h3 id="1-过拟合的原因">1. <strong>过拟合的原因</strong></h3>
<ul>
<li>模型在训练时过于“记住”了训练数据中的细节，而不是学到泛化规律（即如何处理新数据）。</li>
<li>复杂的模型（比如有很多参数）更容易过拟合。</li>
</ul>
<h3 id="2-正则化的目的">2. <strong>正则化的目的</strong></h3>
<p>正则化的目标是通过限制模型的复杂度，避免模型对训练数据的过度拟合。它通过给模型加上一些额外的约束，使得模型在学习时不能“随意”地调整参数，而是保持适度的简单性。</p>
<h3 id="3-如何实现正则化？">3. <strong>如何实现正则化？</strong></h3>
<p>正则化通常通过在损失函数中加入一个额外的项来实现，这个项通常与模型的<strong>权重</strong>（参数）大小有关。常见的正则化方法有两种：</p>
<h4 id="1-L1-正则化（Lasso）">(1) <strong>L1 正则化（Lasso）</strong></h4>
<ul>
<li>L1 正则化在损失函数中加入一个与<strong>权重的绝对值之和</strong>成比例的项。</li>
<li>它的作用是“惩罚”大权重，迫使模型学习到的特征更加简洁。</li>
<li><strong>L1 正则化的效果</strong>：它可以将某些权重变为零，相当于<strong>自动选择特征</strong>。</li>
</ul>
<p>公式：<br>
$$<br>
\text{L1 正则化} = \lambda \sum_{i=1}^{n} |w_i|<br>
$$</p>
<ul>
<li>其中，$w_i$ 是模型的权重，$\lambda$ 是正则化的强度（越大，惩罚越强）。</li>
</ul>
<h4 id="2-L2-正则化（Ridge）">(2) <strong>L2 正则化（Ridge）</strong></h4>
<ul>
<li>L2 正则化在损失函数中加入一个与<strong>权重的平方和</strong>成比例的项。</li>
<li>它的作用是<strong>惩罚大权重</strong>，但不像 L1 那样将权重变为零，而是让权重变得更小，防止某些特征对模型的影响过大。</li>
<li><strong>L2 正则化的效果</strong>：它可以平滑模型，使模型更加稳健。</li>
</ul>
<p>公式：<br>
$$<br>
\text{L2 正则化} = \lambda \sum_{i=1}^{n} w_i^2<br>
$$</p>
<h3 id="4-如何理解正则化？">4. <strong>如何理解正则化？</strong></h3>
<p><strong>举个简单的例子</strong>：<br>
假设你在做一个学生的成绩预测模型，输入特征有 <strong>学习时间、上课出勤率、平时作业分数</strong> 等。如果模型过于复杂，可能会学习到一些不重要的特征，比如学生每天吃的零食种类。这样，模型会在训练集上表现很好，但在实际使用时可能表现差，因为这些不重要的特征对新数据没有帮助。</p>
<p>通过正则化，你给模型添加了一些<strong>惩罚</strong>，强迫它只关注重要的特征，并避免关注那些噪音和不重要的细节。这样，模型会学到更有用的规律，泛化能力更强。</p>
<h3 id="5-正则化的效果">5. <strong>正则化的效果</strong></h3>
<ul>
<li><strong>减少过拟合</strong>：通过限制模型复杂度，使得模型不仅能在训练数据上表现好，也能在新数据上表现好。</li>
<li><strong>提升模型的泛化能力</strong>：正则化可以帮助模型学到更为通用的规律，而不是记住训练数据中的细节。</li>
</ul>
<h2 id="如何理解L1-L2正则化？">如何理解L1/L2正则化？</h2>
<p>好的，我们再深入一点，详细计算一下 L1 正则化是如何惩罚权重的，以及它如何引导模型减少不重要特征的影响。</p>
<h3 id="假设情境">假设情境</h3>
<p>我们使用一个非常简单的线性回归模型，有 <strong>2 个特征</strong>$x_1$ 和$x_2$，目标是预测 <strong>y</strong>。模型的公式为：<br>
$$<br>
y = w_1 x_1 + w_2 x_2 + b<br>
$$<br>
其中：</p>
<ul>
<li>$w_1$ 和 $w_2$ 是特征 $x_1$ 和 $x_2$ 的权重。</li>
<li>$b$ 是偏置项（在这里我们忽略不讨论）。</li>
</ul>
<p>我们要最小化的是<strong>总损失</strong>，即：<br>
$$<br>
\text{总损失} = \text{MSE（均方误差）} + \text{L1 正则化损失}<br>
$$<br>
均方误差（MSE）用来衡量模型预测值与实际值之间的误差，而 L1 正则化损失用来惩罚权重，使它们尽可能小。</p>
<h3 id="1-没有正则化的情况">1. <strong>没有正则化的情况</strong></h3>
<p>假设我们没有正则化，只有 MSE 损失。对于一个简单的回归模型，损失函数可以表示为：<br>
$$<br>
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_{\text{预测}, i} - y_{\text{实际}, i})^2<br>
$$<br>
其中，$y_{\text{预测}, i} = w_1 x_{1,i} + w_2 x_{2,i} + b$，是模型的预测值。</p>
<p>在没有正则化的情况下，模型的目标是尽量减小 MSE，但它不考虑特征权重的大小，可能会导致模型过度依赖某些特征。</p>
<h3 id="2-加入-L1-正则化">2. <strong>加入 L1 正则化</strong></h3>
<p>L1 正则化在损失函数中加入了对权重的惩罚项。对于每个权重，惩罚项是它的绝对值。损失函数现在变成了：<br>
$$<br>
\text{总损失} = \text{MSE} + \lambda \left( |w_1| + |w_2| \right)<br>
$$<br>
其中，$\lambda$ 是正则化系数，控制惩罚项的强度。如果 $\lambda$ 很大，惩罚就会更强，权重会被压缩得更小。</p>
<h3 id="3-例子：权重调整过程">3. <strong>例子：权重调整过程</strong></h3>
<p>假设我们训练了模型，得到了初始的权重：</p>
<ul>
<li>$w_1 = 5$</li>
<li>$w_2 = 0.1$</li>
<li>正则化系数 $\lambda = 0.1$</li>
</ul>
<p>我们还假设 MSE 损失部分的计算结果为 50（为了简化计算，不考虑具体的样本数据）。</p>
<h4 id="步骤-1：计算正则化损失">步骤 1：计算正则化损失</h4>
<p>L1 正则化损失是：<br>
$$<br>
\text{L1 正则化损失} = \lambda \left( |w_1| + |w_2| \right)<br>
$$<br>
代入权重值：<br>
$$<br>
\text{L1 正则化损失} = 0.1 \times (|5| + |0.1|) = 0.1 \times (5 + 0.1) = 0.1 \times 5.1 = 0.51<br>
$$</p>
<h4 id="步骤-2：总损失">步骤 2：总损失</h4>
<p>现在，模型的总损失是：<br>
$$<br>
\text{总损失} = \text{MSE} + \text{L1 正则化损失}<br>
$$<br>
$$<br>
\text{总损失} = 50 + 0.51 = 50.51<br>
$$<br>
所以，在这种情况下，L1 正则化的惩罚项增加了总损失。</p>
<h4 id="步骤-3：权重更新（梯度下降）">步骤 3：权重更新（梯度下降）</h4>
<p>模型通过梯度下降来最小化总损失。在梯度下降过程中，L1 正则化的惩罚项会影响梯度的计算，使得权重的更新不仅考虑误差，还考虑惩罚项。</p>
<p>在每次更新过程中，L1 正则化对每个权重的影响如下：</p>
<ul>
<li>对于 $w_1$，L1 正则化会迫使它变小。因为 $|w_1| = 5$，所以它的梯度下降会考虑惩罚项。</li>
<li>对于 $w_2$，L1 正则化会更加显著地影响它。由于 $|w_2| = 0.1$，惩罚项在 $w_2$ 上的效果更明显，权重会逐步变小。</li>
</ul>
<p><strong>因为较大的权重已经对模型的预测结果产生较大的影响，L1 正则化的惩罚项只是对这个权重施加一个适度的惩罚，导致它会被逐渐减小，但不至于过多压缩。较小的权重这是因为它本身的绝对值较小，正则化的惩罚就会占据更大的比重，迫使它迅速变小甚至归零。</strong>（模型总是在选择使得损失函数变小的参数）</p>
<h4 id="步骤-4：逐步更新">步骤 4：逐步更新</h4>
<p>假设我们计算出了梯度，并在一次更新后得到以下新的权重（梯度下降步长为 0.1）：</p>
<ul>
<li>$ w_1 = 4.8$</li>
<li>$w_2 = 0.05$</li>
</ul>
<p>这个更新过程说明了，<strong>L1 正则化会推动较小的权重变得更小</strong>，并让它们趋向于零。</p>
<h3 id="4-最终权重的变化">4. <strong>最终权重的变化</strong></h3>
<p>继续进行多次更新，L1 正则化会导致 $w_2$（那个较小的权重）逐渐减少，甚至变为零。假设经过足够多的训练步骤，最后我们得到：</p>
<ul>
<li>$w_1  = 4.5$</li>
<li>$w_2 = 0$</li>
</ul>
<p>此时，<strong>L1 正则化已经将不重要的特征（即 $w_2$）的权重压缩为零</strong>，表明模型已经“去除了”不重要的特征。</p>
<h2 id="自注意力机制">自注意力机制</h2>
<h3 id="自注意力机制-Self-Attention-Mechanism">自注意力机制 (Self-Attention Mechanism)</h3>
<p>自注意力机制是 Transformer 模型的核心，它帮助模型捕捉输入序列中不同位置之间的依赖关系。它的目的是使每个输入元素（例如一个词）能够与其他输入元素进行交互，从而更好地理解上下文信息。自注意力机制比传统的循环神经网络（RNN）和卷积神经网络（CNN）更强大，因为它能够同时考虑整个序列的信息，而不需要按顺序逐一处理。</p>
<h4 id="核心概念">核心概念</h4>
<ol>
<li>
<p><strong>输入表示</strong>：自注意力机制首先接收一个输入序列。对于文本输入，通常每个词被嵌入成一个向量，形成一个矩阵。</p>
</li>
<li>
<p><strong>查询（Query）、键（Key）和值（Value）</strong>：</p>
<ul>
<li><strong>查询（Q）</strong>：每个输入词（或词向量）都会映射为一个查询向量，用来与其他词进行“匹配”。</li>
<li><strong>键（K）</strong>：每个输入词也会映射为一个键向量，查询会与这些键进行匹配，以确定其相关性。</li>
<li><strong>值（V）</strong>：每个输入词也会有一个值向量，最终输出是基于查询和键的匹配得分加权得到的值向量的加权和。</li>
</ul>
</li>
<li>
<p><strong>计算注意力得分</strong>：<br>
自注意力机制通过计算每个查询向量与所有键向量之间的相似度（通常使用点积）。相似度得分越高，表示查询和键之间的关联越强。通常，通过Softmax对得分进行归一化，得到注意力权重。</p>
<p>$$<br>
\text{Attention Score} = Q \cdot K^T<br>
$$<br>
然后，使用 Softmax 函数将这些得分转化为概率，确保权重总和为1：</p>
<p>$$<br>
\text{Attention Weight} = \text{Softmax}(Q \cdot K^T)<br>
$$</p>
</li>
<li>
<p><strong>加权求和值</strong>：<br>
通过对每个值（V）向量加权求和，生成最终的输出。具体来说，注意力权重会决定各个值向量在最终输出中的贡献。</p>
<p>$$<br>
\text{Output} = \sum (\text{Attention Weight}_i \cdot V_i)<br>
$$</p>
</li>
<li>
<p><strong>多头注意力</strong>：<br>
Transformer 使用多头注意力机制，意味着查询、键和值向量会被拆分成多个子空间，分别计算注意力。这允许模型在不同的子空间中关注输入的不同方面。</p>
<p>最后，多个头的结果会被拼接起来，并通过一个线性层进行变换。</p>
</li>
</ol>
<h4 id="自注意力机制的作用">自注意力机制的作用</h4>
<ul>
<li><strong>捕获长距离依赖</strong>：传统的 RNN 和 CNN 主要通过局部邻域来获取信息，而自注意力机制可以通过计算所有词的相关性，捕获长距离依赖。</li>
<li><strong>并行处理</strong>：由于不依赖于序列的顺序，自注意力机制允许模型对输入进行并行处理，效率更高。</li>
<li><strong>灵活性</strong>：每个位置根据上下文信息调整其关注的焦点。</li>
</ul>
<h3 id="Transformer-中的自注意力层">Transformer 中的自注意力层</h3>
<p>Transformer 模型完全基于自注意力机制，它使用了多个自注意力层来处理序列数据。Transformer 模型中最重要的组件是 <strong>自注意力层</strong>（Self-Attention Layer）和 <strong>前馈神经网络</strong>（Feed-Forward Neural Networks）。我们来深入探讨自注意力层的工作原理。</p>
<h4 id="Transformer-模型架构">Transformer 模型架构</h4>
<p>Transformer 模型由多个堆叠的编码器（Encoder）和解码器（Decoder）组成。在每个编码器和解码器中，自注意力层都起着关键作用。</p>
<ol>
<li>
<p><strong>编码器（Encoder）</strong>：<br>
每个编码器层有两个主要部分：</p>
<ul>
<li><strong>自注意力机制</strong>：编码器的每一层都包括一个自注意力层，它允许输入序列中的每个位置根据全局上下文来调整其表示。</li>
<li><strong>前馈神经网络</strong>：每个编码器还包含一个小型的前馈神经网络（Feed-Forward Neural Network），它通过线性变换和激活函数进一步处理输入。</li>
</ul>
</li>
<li>
<p><strong>解码器（Decoder）</strong>：<br>
解码器的结构与编码器相似，但它在自注意力机制中加入了“遮蔽”（Masked）机制，以确保解码时每个位置仅能看到当前位置及之前的输入。此外，解码器中的每个自注意力层还会与编码器的输出交互，获取全局信息。</p>
</li>
</ol>
<h4 id="自注意力层的具体实现">自注意力层的具体实现</h4>
<p>自注意力层包括以下几个步骤：</p>
<ol>
<li>
<p><strong>输入嵌入和位置编码</strong>：<br>
输入的每个词首先通过嵌入（embedding）层转换为向量，并加上位置编码，以捕捉序列的顺序信息。</p>
</li>
<li>
<p><strong>查询、键和值</strong>：<br>
将输入的嵌入向量通过线性变换生成查询（Q）、键（K）和值（V）向量。</p>
</li>
<li>
<p><strong>计算注意力</strong>：<br>
计算每个查询和所有键的点积，得到注意力分数，并通过 Softmax 得到归一化的权重。</p>
</li>
<li>
<p><strong>加权求和</strong>：<br>
使用这些权重对值（V）向量加权求和，得到每个位置的加权表示。</p>
</li>
<li>
<p><strong>输出</strong>：<br>
最后的输出通过多头注意力机制合并多个头的结果，然后通过一个线性层进行变换。</p>
</li>
</ol>
<h4 id="自注意力层的公式">自注意力层的公式</h4>
<p>假设我们有一个输入序列 $X = [x_1, x_2, \dots, x_n]$，每个 $x_i$ 是一个词的嵌入向量。通过训练，我们学习到查询（Q）、键（K）和值（V）矩阵，分别将输入映射为 $Q = XW_Q$、$K = XW_K$、$V = XW_V$。</p>
<ol>
<li><strong>计算注意力得分</strong>：</li>
</ol>
<p>$$<br>
\text{Attention Score}(Q, K) = \frac{QK^T}{\sqrt{d_k}}<br>
$$</p>
<ol start="2">
<li><strong>归一化注意力得分</strong>：</li>
</ol>
<p>$$<br>
\text{Attention Weight} = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)<br>
$$</p>
<ol start="3">
<li><strong>加权求和</strong>：</li>
</ol>
<p>$$<br>
\text{Output} = \text{Attention Weight} \cdot V<br>
$$</p>
<h2 id="多头注意力机制">多头注意力机制</h2>
<p>在 Transformer 中的 <strong>多头注意力</strong>（Multi-Head Attention）机制中，“拼接”（concatenation）指的是将多个注意力头的输出连接起来，以便能够捕获输入序列的更多信息。接下来，拼接后的结果会经过一个线性变换来得到最终的输出。</p>
<h3 id="多头注意力机制的步骤">多头注意力机制的步骤</h3>
<ol>
<li>
<p><strong>计算多个注意力头</strong>：<br>
在多头注意力中，首先将输入的查询（Query）、键（Key）和值（Value）通过不同的线性变换分别映射到多个子空间，形成多个不同的查询、键和值的集合。每个头（head）对应于不同的查询、键、值子空间。</p>
<p>假设总的嵌入维度是 $C$，而有 $h$ 个注意力头。那么，每个注意力头的维度为 $A = \frac{C}{h}$，即每个头的维度是总维度 $C$ 除以头的数量 $h$。</p>
</li>
<li>
<p><strong>每个注意力头的计算</strong>：<br>
对于每个注意力头，都会计算一个独立的自注意力输出。具体的计算过程如下：</p>
<ul>
<li>对每个头，使用自己的查询、键和值计算注意力得分并进行加权求和，得到该头的输出。</li>
</ul>
</li>
<li>
<p><strong>拼接所有头的输出</strong>：<br>
每个头的输出维度是 $A$，所以如果有 $h$ 个头，那么拼接后的输出将是一个维度为 $h \times A = C$ 的向量。也就是说，拼接后的结果恢复了原始的输入维度 $C$。</p>
<p>这个拼接的过程非常简单，就是将每个头的输出按顺序连接在一起，得到一个大向量。</p>
</li>
<li>
<p><strong>线性变换</strong>：<br>
将拼接后的多头注意力输出通过一个线性层（即一个权重矩阵）进行变换，得到最终的输出。</p>
</li>
</ol>
<h3 id="举个例子-2">举个例子</h3>
<p>假设输入的维度为 $C = 8$，有 $h = 2$ 个注意力头（即每个头的维度 $A = 4$）。我们来看看这个过程如何进行：</p>
<ol>
<li>
<p><strong>输入和映射</strong>：<br>
输入是一个长度为 8 的向量，经过线性变换后，我们会得到两个查询（Query）、键（Key）和值（Value）的子空间，每个子空间的维度是 4。</p>
</li>
<li>
<p><strong>计算每个头的输出</strong>：<br>
对于每个头（总共有 2 个头），计算它们的注意力输出（每个头的输出维度为 4）。</p>
<ul>
<li>头 1 的输出是一个 4 维向量。</li>
<li>头 2 的输出是另一个 4 维向量。</li>
</ul>
</li>
<li>
<p><strong>拼接</strong>：<br>
将这两个头的输出拼接起来，得到一个 8 维的向量：</p>
<p>$$<br>
\text{Concatenated Output} = [\text{Head 1 Output}, \text{Head 2 Output}]<br>
$$</p>
<p>这就是拼接的过程。最终拼接结果的维度是 $C = 8$。</p>
</li>
<li>
<p><strong>线性变换</strong>：<br>
将拼接后的输出通过一个线性变换，得到最终的多头注意力层输出。</p>
</li>
</ol>
<h3 id="总结">总结</h3>
<ul>
<li>多头注意力的拼接步骤其实是将每个注意力头计算出的输出向量沿着维度方向拼接起来，得到一个较大的向量。</li>
<li>这种拼接允许模型在不同的子空间中“并行”地学习不同类型的特征，而不是仅仅依赖于一个单一的注意力头。</li>
<li>最后，通过线性变换将拼接后的输出映射回原始维度 $C$，为后续的网络层提供输入。</li>
</ul>
<p>这个过程使得 Transformer 模型能够从多个角度和不同的子空间捕获序列中的信息，从而提高模型的表达能力。</p>
<h2 id="传统神经网络的输入和transform中的输入">传统神经网络的输入和transform中的输入</h2>
<h3 id="1-传统神经网络中的输入">1. <strong>传统神经网络中的输入</strong></h3>
<p>在传统的神经网络（如前馈神经网络）中，输入通常是一个固定维度的向量或矩阵，具体内容取决于任务类型：</p>
<ul>
<li>
<p><strong>结构化数据</strong>：<br>
输入是一个包含多种特征的向量。例如，在房价预测中，输入可能是房屋面积、房间数量等数值型特征。</p>
</li>
<li>
<p><strong>图像数据</strong>：<br>
输入是二维或三维矩阵。例如，在图像分类任务中，输入是像素值组成的矩阵，可能还有 RGB 通道。</p>
</li>
<li>
<p><strong>文本数据</strong>：<br>
输入通常是经过嵌入的向量，例如词嵌入（word embedding）或句子嵌入。</p>
</li>
</ul>
<h4 id="特点：">特点：</h4>
<ul>
<li>输入是独立的，每个样本的输入没有显式的上下文关联。</li>
<li>输入的维度固定。</li>
<li>输入数据需要提前提取和处理为数值形式。</li>
</ul>
<hr>
<h3 id="2-Transformer-编码器中的前馈神经网络输入">2. <strong>Transformer 编码器中的前馈神经网络输入</strong></h3>
<p>在 Transformer 的编码器中，前馈神经网络（Feed-Forward Neural Network, FFN）的输入是 <strong>注意力层的输出</strong>，它是上下文相关的特征表示。以下是输入的主要特点：</p>
<ul>
<li>
<p><strong>来源</strong>：<br>
输入是经过自注意力层处理后的输出，每个词（或序列位置）的表示已经包含了与其他词的上下文依赖关系。</p>
</li>
<li>
<p><strong>形状</strong>：<br>
输入是一个三维张量，形状为 $[batch_size, sequence_length, embedding_dim]$。</p>
<ul>
<li><code>batch_size</code>：批量大小。</li>
<li><code>sequence_length</code>：序列的长度（例如一个句子中的词数）。</li>
<li><code>embedding_dim</code>：每个词的嵌入向量的维度。</li>
</ul>
</li>
<li>
<p><strong>内容</strong>：<br>
每个位置的表示已经结合了该位置与其他位置之间的注意力权重（即上下文信息），所以它是 <strong>上下文相关的表示</strong>。</p>
</li>
</ul>
<h4 id="特点：-2">特点：</h4>
<ul>
<li>输入包含上下文依赖信息。</li>
<li>输入维度与嵌入维度一致，但输入每个位置的值已经经过注意力机制的加权。</li>
<li>输入可以表示整个序列的信息，而不是单个独立样本。</li>
</ul>
<hr>
<h3 id="3-传统神经网络输入与-Transformer-编码器中-FFN-输入的区别">3. <strong>传统神经网络输入与 Transformer 编码器中 FFN 输入的区别</strong></h3>
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>传统神经网络的输入</strong></th>
<th><strong>Transformer 编码器中的 FFN 输入</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>上下文依赖</strong></td>
<td>输入数据通常是独立的，每个样本相互无关。</td>
<td>输入每个位置的表示包含上下文相关信息。</td>
</tr>
<tr>
<td><strong>输入形状</strong></td>
<td>向量或矩阵，具体形状依赖任务。</td>
<td>三维张量，形状为 ([batch_size, sequence_length, embedding_dim])。</td>
</tr>
<tr>
<td><strong>维度</strong></td>
<td>固定的输入特征维度。</td>
<td>嵌入维度通常与序列的长度和批量大小相关联。</td>
</tr>
<tr>
<td><strong>表示方式</strong></td>
<td>特征通常是直接提取的原始数据特征。</td>
<td>输入是经过嵌入层和自注意力层处理的特征表示。</td>
</tr>
<tr>
<td><strong>上下文信息</strong></td>
<td>特征是独立的，不包含其他样本的信息。</td>
<td>特征表示已经包含序列中其他位置的相关性。</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="4-总结">4. <strong>总结</strong></h3>
<ul>
<li><strong>传统神经网络</strong>：输入是独立的特征向量，通常没有上下文信息。网络依赖于层间权重来建模数据之间的关系。</li>
<li><strong>Transformer 编码器中的前馈神经网络</strong>：输入是注意力层输出的上下文相关表示，已经包含序列中各位置之间的依赖关系。FFN 的作用是在每个位置上进一步非线性地变换这些上下文相关的特征。</li>
</ul>
<p>这种区别使得 Transformer 特别适合处理序列数据，例如自然语言处理和时间序列建模，因为其输入已经通过自注意力层捕获了序列的全局信息。</p>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://kelinkong.github.io/2024/12/27/AI-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0/" data-id="cmkvx67qm000c7oogc7xyguq4" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEgElEQVR42u3aS47rRhAEwHf/S8tbAzY1mVWUzdYEVwNBIruDA3T9/vyJr9fFdfWdv3/yz7/zZ109Pfk8WdXNFyZMmDBheiTT6+31fsMJ3NUnV8vN7zaDeL/+q6djwoQJE6bTmZLbJadkDvTDsuIXmYcC7b4wYcKECROmfAOzo3oWECSvBxMmTJgwYfpcmXVWWc3Dkbw8jQkTJkyYfg9Tcru7Ft3+dvYCZq/5hlo4JkyYMGF6GFN+MH/f3zdfmDBhwoTpYUyv8mrbhJsGZ/75Xcf/5a4xYcKECdOxTG3qO0O5q8jbripvoEaJOiZMmDBhOpapXVxSnG2Lp22BuG1JJi+pTo8xYcKECdPjmWbbKI7Mt4d0O0bTthvfP7cIVjBhwoQJ07FM+wii3Wq+7c1Qzl3F6+i/DBMmTJgwPZipLW7Oirlt0/ETw0OzMAUTJkyYMH0HUzsEs1nuJhS4d3CneAGYMGHChOlYptkhnbcw82Q1T7xnoUCe9F6GEZgwYcKE6SuYZijJgmZHeJ6Q5ynu8J6YMGHChOlwpvxAvWv0Zx8WtAM9s+YoJkyYMGE6l6mlaUHv4s5/1a4fEyZMmDBh2pRo82S43Uye3ObJfD0xhAkTJkyYDmealWI3gzttIt0GBy0iJkyYMGH6DqYZ0HD8ZR2OJE3WZHf1yA4mTJgwYTqQafbjZIntUmbEw+3FrVZMmDBhwnQ6072PSRA3z2rfefvfcblCTJgwYcJ0LNPsIL93q3fhbtqrUTkYEyZMmDAdzpQUTDcpbr642XP33/8BCxMmTJgwfQXT5pDOCdpW6L583O6rGN/BhAkTJkyPZ9onmUl6PBsbzYOM/6hFigkTJkyYjmK6tyQaHaiLb7aHfdvmvNwdJkyYMGE6nKltIubF0xb9PWJeGp6d43WkgAkTJkyYHs+0L9HmCfCshbkvQ88anHWRFxMmTJgwPZJptqW7Cr7JBmZjPffeGRMmTJgwnct0V0r8Cq42l0xeQ9sinY0NYcKECROmc5nyh7XJZMs3+2RGXxeFMWHChAnTsUyzICBf6GbRbUCwCWKKVWHChAkTpgOZPtHI3JdQ94XgNtz5gQ8TJkyYMB3ItB98acu4OXdbwN0P+qxiJUyYMGHC9GCmT7QJ2w5qW8DdFG1rPkyYMGHCdCxTW2D9REE2P3Pz1Ho2GBQlwJgwYcKE6SimWUAwK+a2z0pKzLPRnCQguPw/woQJEyZMxzINo4myjDsrMc+e2Ka4l68fEyZMmDAdy7QH2gzEtCXgPGiYNUGLCxMmTJgwHcL0Kq+21Ls/qjcp6+ZZ/xI3YcKECROmA5lmZ2JezJ19kpSGN8lzvvJV6IAJEyZMmB7DlBzbbeKab6Ad1kmS4eS11eECJkyYMGE6nOn9UvIib44+C0TyDecrj76PCRMmTJh+GVObT7dBRk6Ql4NXwQQmTJgwYfplTPmozawN2ZaPkyJv+8SPxE2YMGHChOl/YvrcEMzszrPBnbwFm68QEyZMmDCdzrQ54GfjMpvkeRPQ7MvKmDBhwoTpKKa/ALk2zhd9stBxAAAAAElFTkSuQmCC">Share</a><div class="tags"></div><div class="post-nav"><a class="pre" href="/2025/01/09/Java-%E7%BC%96%E8%AF%91%E7%9B%B8%E5%85%B3/">Java-编译相关</a><a class="next" href="/2024/12/24/AI-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-NLP-%E5%85%A5%E9%97%A8/">AI-基于transformers的自然语言处理(NLP)入门</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://kelinkong.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img class="nofancybox" src="/img/logo.jpg"/></a><p>Kelin is a boring girl, but not always.</p><a class="info-icon" href="https://github.com/kelinkong" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Digital-Human/">Digital Human</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Frontend/">Frontend</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Network-Security/">Network-Security</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Operating-System/">Operating-System</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Radar-Principle/">Radar Principle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/solutions/">solutions</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/architecture/" style="font-size: 15px;">architecture</a> <a href="/tags/CMU15-445/" style="font-size: 15px;">CMU15-445</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/%E9%9B%B7%E8%BE%BE%E5%8E%9F%E7%90%86/" style="font-size: 15px;">雷达原理</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2026/01/22/%E3%80%90c++%E3%80%91brtc%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E8%AE%B0%E5%BD%95/">【c++】brtc项目开发记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/12/08/Java-copy%E6%A8%A1%E5%BC%8F/">Java-copy模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/11/28/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8-tryHackMe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">网络安全-学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/11/18/Java-%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F/">Java-命令模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/09/11/Java-%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E8%AE%B0%E5%BD%95/">Java-项目开发记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/06/13/%E9%9B%B7%E8%BE%BE%E5%8E%9F%E7%90%86-ukf-%E6%BB%A4%E6%B3%A2%E5%99%A8/">雷达原理-ukf 滤波器</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/06/13/%E9%9B%B7%E8%BE%BE%E5%8E%9F%E7%90%86-%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/">雷达原理-卡尔曼滤波器基础学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/06/05/Java-Java%E7%9A%84%E5%AE%B9%E5%99%A8%E5%92%8CC-%E7%9A%84STL%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AF%B9%E6%AF%94/">Java-Java的容器和C++的STL容器的对比</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/06/05/%E6%95%B0%E5%AD%97%E4%BA%BA-Java%E9%9F%B3%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86-%E9%9F%B3%E9%A2%91%E6%8E%A8%E6%B5%81%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/">数字人-Java音视频处理-音频推流失败问题排查</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/05/21/%E6%95%B0%E5%AD%97%E4%BA%BA-Java%E9%9F%B3%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86/">数字人-Java音视频处理-推流和拉流</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2026 <a href="/." rel="nofollow">Kelin's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/love.js?v=1.0.0"></script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript">window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true
  },
  svg: { fontCache: 'global' },
  startup: {
    pageReady: () => {
      return MathJax.typesetPromise().catch(err => console.log(err))
    }
  }
};</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>