<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>AI-神经网络中的三个基本概念：梯度下降、反向传播、损失函数 | Kelin's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">AI-神经网络中的三个基本概念：梯度下降、反向传播、损失函数</h1><a id="logo" href="/.">Kelin's blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">AI-神经网络中的三个基本概念：梯度下降、反向传播、损失函数</h1><div class="post-meta">2024-12-19<span> | </span><span class="category"><a href="/categories/AI/">AI</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 7.3k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 30</span><span class="post-meta-item-text"> Minutes</span></span></span></div><div class="post-content"><!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

<p>文章参考：<a target="_blank" rel="noopener" href="https://github.com/microsoft/ai-edu/blob/master/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/A2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/%E7%AC%AC1%E6%AD%A5%20-%20%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/02.0-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.md">神经网络中的三个基本概念</a></p>
<h2 id="2-0-通俗地理解三大概念"><a href="#2-0-通俗地理解三大概念" class="headerlink" title="2.0 通俗地理解三大概念"></a>2.0 通俗地理解三大概念</h2><p>这三大概念是：反向传播，梯度下降，损失函数。</p>
<p>神经网络训练的最基本的思想就是：先“猜”一个结果，称为预测结果 $a$，看看这个预测结果和事先标记好的训练集中的真实结果 $y$ 之间的差距，然后调整策略，再试一次，这一次就不是“猜”了，而是有依据地向正确的方向靠近。如此反复多次，一直到预测结果和真实结果之间相差无几，亦即 $|a-y|\rightarrow 0$，就结束训练。</p>
<p>在神经网络训练中，我们把“猜”叫做初始化，可以随机，也可以根据以前的经验给定初始值。即使是“猜”，也是有技术含量的。</p>
<p>下面我们举几个例子来直观的说明下这三个概念。</p>
<h3 id="2-0-1-例一：猜数"><a href="#2-0-1-例一：猜数" class="headerlink" title="2.0.1 例一：猜数"></a>2.0.1 例一：猜数</h3><p>甲乙两个人玩儿猜数的游戏，数字的范围是 $[1,50]$：</p>
<p>甲：我猜5</p>
<p>乙：太小了</p>
<p>甲：50</p>
<p>乙：有点儿大</p>
<p>甲：30</p>
<p>乙：小了</p>
<p>……</p>
<p>在这个游戏里：</p>
<ul>
<li>目的：猜到乙心中的数字；</li>
<li>初始化：甲猜5；</li>
<li>前向计算：甲每次猜的新数字；</li>
<li>损失函数：乙在根据甲猜的数来和自己心中想的数做比较，得出“大了”或“小了”的结论；</li>
<li>反向传播：乙告诉甲“小了”、“大了”；</li>
<li>梯度下降：甲根据乙的反馈中的含义自行调整下一轮的猜测值。</li>
</ul>
<p>这里的损失函数是什么呢？就是“太小了”，“有点儿大”，很不精确！这个“所谓的”损失函数给出了两个信息：</p>
<ol>
<li>方向：大了或小了</li>
<li>程度：“太”，“有点儿”，但是很模糊</li>
</ol>
<h3 id="2-0-2-例二：黑盒子"><a href="#2-0-2-例二：黑盒子" class="headerlink" title="2.0.2 例二：黑盒子"></a>2.0.2 例二：黑盒子</h3><p>假设有一个黑盒子：</p>
<p>我们只能看到输入和输出的数值，看不到里面的样子，当输入1时，输出2.334，然后黑盒子有个信息显示：我需要输出值是4。然后我们试了试输入2，结果输出5.332，一下子比4大了很多。那么我们第一次的损失值是 $2.334-4&#x3D;-1.666$，而二次的损失值是 $5.332-4&#x3D;1.332$。</p>
<p>这里，我们的损失函数就是一个简单的减法，用实际值减去目标值，但是它可以告诉你两个信息：1）方向，是大了还是小了；2）差值，是0.1还是1.1。这样就给了我们下一次猜的依据。</p>
<ul>
<li>目的：猜到一个输入值，使得黑盒子的输出是4；</li>
<li>初始化：输入1；</li>
<li>前向计算：黑盒子内部的数学逻辑；</li>
<li>损失函数：在输出端，用输出值减4；</li>
<li>反向传播：告诉猜数的人差值，包括正负号和值；</li>
<li>梯度下降：在输入端，根据正负号和值，确定下一次的猜测值。</li>
</ul>
<h3 id="2-0-3-例三：打靶"><a href="#2-0-3-例三：打靶" class="headerlink" title="2.0.3 例三：打靶"></a>2.0.3 例三：打靶</h3><p>小明拿了一支步枪，射击100米外的靶子。这支步枪没有准星，或者是准星有问题，或者是小明眼神儿不好看不清靶子，或者是雾很大，或者风很大，或者由于木星的影响而侧向引力场异常……反正就是遇到各种干扰因素。</p>
<p>第一次试枪后，拉回靶子一看，弹着点偏左了，于是在第二次试枪时，小明就会有意识地向右侧偏几毫米，再看靶子上的弹着点，如此反复几次，小明就会掌握这支步枪的脾气了。图2-2显示了小明的5次试枪过程。</p>
<p><img src="/../imgs/image-72.png" alt="图2-2 打靶的弹着点记录"></p>
<p>在有监督的学习中，需要衡量神经网络输出和所预期的输出之间的差异大小。这种误差函数需要能够反映出当前网络输出和实际结果之间一种量化之后的不一致程度，也就是说函数值越大，反映出模型预测的结果越不准确。</p>
<p>这个例子中，小明预期的目标是全部命中靶子的中心，最外圈是1分，之后越向靶子中心分数是2，3，4分，正中靶心可以得10分。</p>
<ul>
<li>每次试枪弹着点和靶心之间的差距就叫做误差，可以用一个误差函数来表示，比如差距的绝对值，如图中的红色线。</li>
<li>一共试枪5次，就是迭代&#x2F;训练了5次的过程 。</li>
<li>每次试枪后，把靶子拉回来看弹着点，然后调整下一次的射击角度的过程，叫做反向传播。注意，把靶子拉回来看和跑到靶子前面去看有本质的区别，后者容易有生命危险，因为还有别的射击者。一个不恰当的比喻是，在数学概念中，人跑到靶子前面去看，叫做正向微分；把靶子拉回来看，叫做反向微分。</li>
<li>每次调整角度的数值和方向，叫做梯度。比如向右侧调整1毫米，或者向左下方调整2毫米。如图中的绿色矢量线。</li>
</ul>
<p>上图是每次单发点射，所以每次训练样本的个数是1。在实际的神经网络训练中，通常需要多个样本，做批量训练，以避免单个样本本身采样时带来的误差。在本例中，多个样本可以描述为连发射击，假设一次可以连打3发子弹，每次的离散程度都类似，如图2-3所示。<br><img src="/../imgs/image-73.png" alt="图2-3 连发弹着点记录"></p>
<ul>
<li>如果每次3发子弹连发，这3发子弹的弹着点和靶心之间的差距之和再除以3，叫做损失，可以用损失函数来表示。</li>
</ul>
<p>那小明每次射击结果和目标之间的差距是多少呢？在这个例子里面，用得分来衡量的话，就是说小明得到的反馈结果从差9分，到差8分，到差2分，到差1分，到差0分，这就是用一种量化的结果来表示小明的射击结果和目标之间差距的方式。也就是误差函数的作用。因为是一次只有一个样本，所以这里采用的是误差函数的称呼。如果一次有多个样本，就要叫做损失函数了。</p>
<p>其实射击还不这么简单，如果是远距离狙击，还要考虑空气阻力和风速，在神经网络里，空气阻力和风速可以对应到隐藏层的概念上。</p>
<p>在这个例子中：</p>
<ul>
<li>目的：打中靶心；</li>
<li>初始化：随便打一枪，能上靶就行，但是要记住当时的步枪的姿态；</li>
<li>前向计算：让子弹飞一会儿，击中靶子；</li>
<li>损失函数：环数，偏离角度；</li>
<li>反向传播：把靶子拉回来看；</li>
<li>梯度下降：根据本次的偏差，调整步枪的射击角度。</li>
</ul>
<p>损失函数的描述是这样的：</p>
<ol>
<li>1环，偏左上45度；</li>
<li>6环，偏左上15度；</li>
<li>7环，偏左；</li>
<li>8环，偏左下15度；</li>
<li>10环。</li>
</ol>
<p>这里的损失函数也有两个信息：</p>
<ol>
<li>距离；</li>
<li>方向。</li>
</ol>
<p><strong>所以，梯度，是个矢量！</strong> 它应该即告诉我们方向，又告诉我们数值。</p>
<h3 id="2-0-4-黑盒子的真正玩法"><a href="#2-0-4-黑盒子的真正玩法" class="headerlink" title="2.0.4 黑盒子的真正玩法"></a>2.0.4 黑盒子的真正玩法</h3><p>以上三个例子比较简单，容易理解，我们把黑盒子再请出来：黑盒子这件事真正的意义并不是猜测当输入是多少时输出会是4。它的实际意义是：我们要破解这个黑盒子！于是，我们会有如下破解流程：</p>
<ol>
<li>记录下所有输入值和输出值，如表2-1。</li>
</ol>
<p>表2-1 样本数据表</p>
<table>
<thead>
<tr>
<th align="center">样本ID</th>
<th>输入(特征值)</th>
<th>输出(标签)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td>1</td>
<td>2.21</td>
</tr>
<tr>
<td align="center">2</td>
<td>1.1</td>
<td>2.431</td>
</tr>
<tr>
<td align="center">3</td>
<td>1.2</td>
<td>2.652</td>
</tr>
<tr>
<td align="center">4</td>
<td>2</td>
<td>4.42</td>
</tr>
</tbody></table>
<ol start="2">
<li>搭建一个神经网络，给出初始权重值，我们先假设这个黑盒子的逻辑是：$z&#x3D;x + x^2$；</li>
<li>输入1，根据 $z&#x3D;x + x^2$ 得到输出为2，而实际的输出值是2.21，则误差值为 $2-2.21&#x3D;-0.21$，小了；</li>
<li>调整权重值，比如 $z&#x3D;1.5x+x^2$，再输入1.1，得到的输出为2.86，实际输出为2.431，则误差值为 $2.86-2.431&#x3D;0.429$，大了；</li>
<li>调整权重值，比如 $z&#x3D;1.2x+x^2$，再输入1.2……</li>
<li>调整权重值，再输入2……</li>
<li>所有样本遍历一遍，计算平均的损失函数值；</li>
<li>依此类推，重复3，4，5，6过程，直到损失函数值小于一个指标，比如 $0.001$，我们就可以认为网络训练完毕，黑盒子“破解”了，实际是被复制了，因为神经网络并不能得到黑盒子里的真实函数体，而只是近似模拟。</li>
</ol>
<p>从上面的过程可以看出，如果误差值是正数，我们就把权重降低一些；如果误差值为负数，则升高权重。</p>
<h3 id="2-0-5-总结"><a href="#2-0-5-总结" class="headerlink" title="2.0.5 总结"></a>2.0.5 总结</h3><p>简单总结一下反向传播与梯度下降的基本工作原理：</p>
<ol>
<li>初始化；</li>
<li>正向计算；</li>
<li>损失函数为我们提供了计算损失的方法；</li>
<li>梯度下降是在损失函数基础上向着损失最小的点靠近而指引了网络权重调整的方向；</li>
<li>反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重；</li>
<li>Go to 2，直到精度足够好（比如损失函数值小于 $0.001$）。</li>
</ol>
<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

<h2 id="2-1-线性反向传播"><a href="#2-1-线性反向传播" class="headerlink" title="2.1 线性反向传播"></a>2.1 线性反向传播</h2><h3 id="2-1-1-正向计算的实例"><a href="#2-1-1-正向计算的实例" class="headerlink" title="2.1.1 正向计算的实例"></a>2.1.1 正向计算的实例</h3><p>假设有一个函数：</p>
<p>$$z &#x3D; x \cdot y \tag{1}$$</p>
<p>其中:</p>
<p>$$x &#x3D; 2w + 3b \tag{2}$$</p>
<p>$$y &#x3D; 2b + 1 \tag{3}$$</p>
<p>计算图如图2-4。</p>
<p><img src="/../imgs/image-74.png" alt="图2-4 简单线性计算的计算图"></p>
<p>注意这里 $x,y,z$ 不是变量，只是中间计算结果；$w,b$ 才是变量。因为在后面要学习的神经网络中，要最终求解的目标是 $w$ 和 $b$ 的值，所以在这里先预热一下。</p>
<p>当 $w &#x3D; 3, b &#x3D; 4$ 时，会得到图2-5的结果。</p>
<p><img src="/../imgs/image-75.png" alt="图2-5 计算结果"></p>
<p>最终的 $z$ 值，受到了前面很多因素的影响：变量 $w$，变量 $b$，计算式 $x$，计算式 $y$。</p>
<h3 id="2-1-2-反向传播求解-w"><a href="#2-1-2-反向传播求解-w" class="headerlink" title="2.1.2 反向传播求解 $w$"></a>2.1.2 反向传播求解 $w$</h3><h4 id="求-w-的偏导"><a href="#求-w-的偏导" class="headerlink" title="求 $w$ 的偏导"></a>求 $w$ 的偏导</h4><p>目前 $z&#x3D;162$，如果想让 $z$ 变小一些，比如目标是 $z&#x3D;150$，$w$ 应该如何变化呢？为了简化问题，先只考虑改变 $w$ 的值，而令 $b$ 值固定为 $4$。</p>
<p>如果想解决这个问题，最笨的办法是可以在输入端一点一点的试，把 $w$ 变成 $3.5$ 试试，再变成 $3$ 试试……直到满意为止。现在我们将要学习一个更好的解决办法：反向传播。</p>
<p>从 $z$ 开始一层一层向回看，图中各节点关于变量 $w$ 的偏导计算结果如下：</p>
<p>因为 $z &#x3D; x \cdot y$，其中 $x &#x3D; 2w + 3b, y &#x3D; 2b + 1$</p>
<p>所以：</p>
<p>$$<br>\frac{\partial{z}}{\partial{w}}&#x3D;\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{w}}&#x3D;y \cdot 2&#x3D;18 \tag{4}<br>$$</p>
<p>其中：</p>
<p>$$<br>\frac{\partial{z}}{\partial{x}}&#x3D;\frac{\partial{}}{\partial{x}}(x \cdot y)&#x3D;y&#x3D;9<br>$$</p>
<p>$$<br>\frac{\partial{x}}{\partial{w}}&#x3D;\frac{\partial{}}{\partial{w}}(2w+3b)&#x3D;2<br>$$</p>
<p><img src="/../imgs/image-76.png" alt="图2-6 对 $w$ 的偏导求解过程"></p>
<p>图2-6其实就是链式法则的具体表现，$z$ 的误差通过中间的 $x$ 传递到 $w$。如果不是用链式法则，而是直接用 $z$ 的表达式计算对 $w$ 的偏导数，会怎么样呢？我们来试验一下。</p>
<p>根据公式1、2、3，我们有：</p>
<p>$$<br>z&#x3D;x \cdot y&#x3D;(2w+3b)(2b+1)&#x3D;4wb+2w+6b^2+3b \tag{5}<br>$$</p>
<p>对上式求 $w$ 的偏导：</p>
<p>$$<br>\frac{\partial z}{\partial w}&#x3D;4b+2&#x3D;4 \cdot 4 + 2&#x3D;18 \tag{6}<br>$$</p>
<p>公式4和公式6的结果完全一致！所以，请大家相信链式法则的科学性。</p>
<h4 id="求-w-的具体变化值"><a href="#求-w-的具体变化值" class="headerlink" title="求 $w$ 的具体变化值"></a>求 $w$ 的具体变化值</h4><p><strong>公式4和公式6的含义是：当 $w$ 变化一点点时，$z$ 会产生 $w$ 的变化值18倍的变化</strong>。记住我们的目标是让 $z&#x3D;150$，目前在初始状态时是 $z&#x3D;162$，所以，问题转化为：当需要 $z$ 从 $162$ 变到 $150$ 时，$w$ 需要变化多少？</p>
<p>既然：</p>
<p>$$<br>\Delta z &#x3D; 18 \cdot \Delta w<br>$$</p>
<p>则：</p>
<p>$$<br>\Delta w &#x3D; {\Delta z \over 18}&#x3D;\frac{162-150}{18}&#x3D; 0.6667<br>$$</p>
<p>所以：</p>
<p>$$<br>w &#x3D; w - 0.6667&#x3D;2.3333<br>$$</p>
<p>$$<br>x&#x3D;2w+3b&#x3D;16.6667<br>$$</p>
<p>$$<br>z&#x3D;x \cdot y&#x3D;16.6667 \times 9&#x3D;150.0003<br>$$</p>
<p>我们一下子就成功地让 $z$ 值变成了 $150.0003$，与 $150$ 的目标非常地接近，这就是偏导数的威力所在。</p>
<h4 id="【课堂练习】推导-z-对-b-的偏导数，结果在下一小节中使用"><a href="#【课堂练习】推导-z-对-b-的偏导数，结果在下一小节中使用" class="headerlink" title="【课堂练习】推导 $z$ 对 $b$ 的偏导数，结果在下一小节中使用"></a>【课堂练习】推导 $z$ 对 $b$ 的偏导数，结果在下一小节中使用</h4><h3 id="2-1-3-反向传播求解-b"><a href="#2-1-3-反向传播求解-b" class="headerlink" title="2.1.3 反向传播求解 $b$"></a>2.1.3 反向传播求解 $b$</h3><h4 id="求-b-的偏导"><a href="#求-b-的偏导" class="headerlink" title="求 $b$ 的偏导"></a>求 $b$ 的偏导</h4><p>这次我们令 $w$ 的值固定为 $3$，变化 $b$ 的值，目标还是让 $z&#x3D;150$。同上一小节一样，先求 $b$ 的偏导数。</p>
<p>注意，在上一小节中，求 $w$ 的导数只经过了一条路：从 $z$ 到 $x$ 到 $w$。但是求 $b$ 的导数时要经过两条路，如图2-7所示：</p>
<ol>
<li>从 $z$ 到 $x$ 到 $b$；</li>
<li>从 $z$ 到 $y$ 到 $b$。</li>
</ol>
<p><img src="/../imgs/image-77.png" alt="图2-7 对b的偏导求解过程"></p>
<p>从复合导数公式来看，这两者应该是相加的关系，所以有：</p>
<p>$$<br>\frac{\partial{z}}{\partial{b}}&#x3D;\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{b}}+\frac{\partial{z}}{\partial{y}}\cdot\frac{\partial{y}}{\partial{b}}&#x3D;y \cdot 3+x \cdot 2&#x3D;63 \tag{7}<br>$$</p>
<p>其中：</p>
<p>$$<br>\frac{\partial{z}}{\partial{x}}&#x3D;\frac{\partial{}}{\partial{x}}(x \cdot y)&#x3D;y&#x3D;9<br>$$</p>
<p>$$<br>\frac{\partial{z}}{\partial{y}}&#x3D;\frac{\partial{}}{\partial{y}}(x \cdot y)&#x3D;x&#x3D;18<br>$$</p>
<p>$$<br>\frac{\partial{x}}{\partial{b}}&#x3D;\frac{\partial{}}{\partial{b}}(2w+3b)&#x3D;3<br>$$</p>
<p>$$<br>\frac{\partial{y}}{\partial{b}}&#x3D;\frac{\partial{}}{\partial{b}}(2b+1)&#x3D;2<br>$$</p>
<p>我们不妨再验证一下链式求导的正确性。把公式5再拿过来：</p>
<p>$$<br>z&#x3D;x \cdot y&#x3D;(2w+3b)(2b+1)&#x3D;4wb+2w+6b^2+3b \tag{5}<br>$$</p>
<p>对上式求b的偏导：</p>
<p>$$<br>\frac{\partial z}{\partial b}&#x3D;4w+12b+3&#x3D;12+48+3&#x3D;63 \tag{8}<br>$$</p>
<p>结果和公式7的链式法则一样。</p>
<h4 id="求-b-的具体变化值"><a href="#求-b-的具体变化值" class="headerlink" title="求 $b$ 的具体变化值"></a>求 $b$ 的具体变化值</h4><p>公式7和公式8的含义是：当 $b$ 变化一点点时，$z$ 会发生 $b$ 的变化值 $63$ 倍的变化。记住我们的目标是让 $z&#x3D;150$，目前在初始状态时是 $162$，所以，问题转化为：当我们需要 $z$ 从 $162$ 变到 $150$ 时，$b$ 需要变化多少？</p>
<p>既然：</p>
<p>$$<br>\Delta z &#x3D; 63 \cdot \Delta b<br>$$</p>
<p>则：</p>
<p>$$<br>\Delta b &#x3D; \frac{\Delta z}{63}&#x3D;\frac{162-150}{63}&#x3D;0.1905<br>$$</p>
<p>所以：<br>$$<br>b&#x3D;b-0.1905&#x3D;3.8095<br>$$</p>
<p>$$<br>x&#x3D;2w+3b&#x3D;17.4285<br>$$</p>
<p>$$<br>y&#x3D;2b+1&#x3D;8.619<br>$$</p>
<p>$$<br>z&#x3D;x \cdot y&#x3D;17.4285 \times 8.619&#x3D;150.2162<br>$$</p>
<p>这个结果也是与 $150$ 很接近了，但是精度还不够。再迭代几次，直到误差不大于 <code>1e-4</code> 时，我们就可以结束迭代了，对于计算机来说，这些运算的执行速度很快。</p>
<h4 id="【课题练习】请自己尝试手动继续迭代两次，看看误差的精度可以达到多少？"><a href="#【课题练习】请自己尝试手动继续迭代两次，看看误差的精度可以达到多少？" class="headerlink" title="【课题练习】请自己尝试手动继续迭代两次，看看误差的精度可以达到多少？"></a>【课题练习】请自己尝试手动继续迭代两次，看看误差的精度可以达到多少？</h4><p>这个问题用数学公式倒推求解一个二次方程，就能直接得到准确的b值吗？是的！但是我们是要说明机器学习的方法，机器并不会解二次方程，而且很多时候不是用二次方程就能解决实际问题的。而上例所示，是用机器所擅长的迭代计算的方法来不断逼近真实解，这就是机器学习的真谛！而且这种方法是普遍适用的。</p>
<h3 id="2-1-4-同时求解-w-和-b-的变化值"><a href="#2-1-4-同时求解-w-和-b-的变化值" class="headerlink" title="2.1.4 同时求解 $w$ 和 $b$ 的变化值"></a>2.1.4 同时求解 $w$ 和 $b$ 的变化值</h3><p>这次我们要同时改变 $w$ 和 $b$，到达最终结果为 $z&#x3D;150$ 的目的。</p>
<p>已知 $\Delta z&#x3D;12$，我们不妨把这个误差的一半算在 $w$ 的账上，另外一半算在 $b$ 的账上：</p>
<p>$$<br>\Delta b&#x3D;\frac{\Delta z &#x2F; 2}{63} &#x3D; \frac{12&#x2F;2}{63}&#x3D;0.095<br>$$</p>
<p>$$<br>\Delta w&#x3D;\frac{\Delta z &#x2F; 2}{18} &#x3D; \frac{12&#x2F;2}{18}&#x3D;0.333<br>$$</p>
<ul>
<li>$w &#x3D; w-\Delta w&#x3D;3-0.333&#x3D;2.667$</li>
<li>$b &#x3D; b - \Delta b&#x3D;4-0.095&#x3D;3.905$</li>
<li>$x&#x3D;2w+3b&#x3D;2 \times 2.667+3 \times 3.905&#x3D;17.049$</li>
<li>$y&#x3D;2b+1&#x3D;2 \times 3.905+1&#x3D;8.81$</li>
<li>$z&#x3D;x \times y&#x3D;17.049 \times 8.81&#x3D;150.2$</li>
</ul>
<h4 id="【课堂练习】用Python代码实现以上双变量的反向传播计算过程"><a href="#【课堂练习】用Python代码实现以上双变量的反向传播计算过程" class="headerlink" title="【课堂练习】用Python代码实现以上双变量的反向传播计算过程"></a>【课堂练习】用Python代码实现以上双变量的反向传播计算过程</h4><p>容易出现的问题：</p>
<ol>
<li>在检查 $\Delta z$ 时的值时，注意要用绝对值，因为有可能是个负数</li>
<li>在计算 $\Delta b$ 和 $\Delta w$ 时，第一次时，它们对 $z$ 的贡献值分别是 $1&#x2F;63$ 和 $1&#x2F;18$，但是第二次时，由于 $b,w$ 值的变化，对 $z$ 的贡献值也会有微小变化，所以要重新计算。具体解释如下：</li>
</ol>
<p>$$<br>\frac{\partial{z}}{\partial{b}}&#x3D;\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{b}}+\frac{\partial{z}}{\partial{y}}\cdot\frac{\partial{y}}{\partial{b}}&#x3D;y \cdot 3+x \cdot 2&#x3D;3y+2x<br>$$</p>
<p>$$<br>\frac{\partial{z}}{\partial{w}}&#x3D;\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{w}}+\frac{\partial{z}}{\partial{y}}\cdot\frac{\partial{y}}{\partial{w}}&#x3D;y \cdot 2+x \cdot 0 &#x3D; 2y<br>$$<br>所以，在每次迭代中，要重新计算下面两个值：<br>$$<br>\Delta b&#x3D;\frac{\Delta z}{3y+2x}<br>$$</p>
<p>$$<br>\Delta w&#x3D;\frac{\Delta z}{2y}<br>$$</p>
<p>以下是程序的运行结果。</p>
<p>没有在迭代中重新计算 $\Delta b$ 的贡献值：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">single</span> variable: b -----</span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">4</span>.<span class="number">000000</span>,z=<span class="number">162</span>.<span class="number">000000</span>,delta_z=<span class="number">12</span>.<span class="number">000000</span></span><br><span class="line"><span class="attribute">delta_b</span>=<span class="number">0</span>.<span class="number">190476</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">3</span>.<span class="number">809524</span>,z=<span class="number">150</span>.<span class="number">217687</span>,delta_z=<span class="number">0</span>.<span class="number">217687</span></span><br><span class="line"><span class="attribute">delta_b</span>=<span class="number">0</span>.<span class="number">003455</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">3</span>.<span class="number">806068</span>,z=<span class="number">150</span>.<span class="number">007970</span>,delta_z=<span class="number">0</span>.<span class="number">007970</span></span><br><span class="line"><span class="attribute">delta_b</span>=<span class="number">0</span>.<span class="number">000127</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">3</span>.<span class="number">805942</span>,z=<span class="number">150</span>.<span class="number">000294</span>,delta_z=<span class="number">0</span>.<span class="number">000294</span></span><br><span class="line"><span class="attribute">delta_b</span>=<span class="number">0</span>.<span class="number">000005</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">3</span>.<span class="number">805937</span>,z=<span class="number">150</span>.<span class="number">000011</span>,delta_z=<span class="number">0</span>.<span class="number">000011</span></span><br><span class="line"><span class="attribute">delta_b</span>=<span class="number">0</span>.<span class="number">000000</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">3</span>.<span class="number">805937</span>,z=<span class="number">150</span>.<span class="number">000000</span>,delta_z=<span class="number">0</span>.<span class="number">000000</span></span><br><span class="line"><span class="attribute">done</span>!</span><br><span class="line"><span class="attribute">final</span> b=<span class="number">3</span>.<span class="number">805937</span></span><br></pre></td></tr></table></figure>
<p>在每次迭代中都重新计算 $\Delta b$ 的贡献值：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">single</span> variable new: b -----</span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">4</span>.<span class="number">000000</span>,z=<span class="number">162</span>.<span class="number">000000</span>,delta_z=<span class="number">12</span>.<span class="number">000000</span></span><br><span class="line"><span class="attribute">factor_b</span>=<span class="number">63</span>.<span class="number">000000</span>, delta_b=<span class="number">0</span>.<span class="number">190476</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">3</span>.<span class="number">809524</span>,z=<span class="number">150</span>.<span class="number">217687</span>,delta_z=<span class="number">0</span>.<span class="number">217687</span></span><br><span class="line"><span class="attribute">factor_b</span>=<span class="number">60</span>.<span class="number">714286</span>, delta_b=<span class="number">0</span>.<span class="number">003585</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">3</span>.<span class="number">805938</span>,z=<span class="number">150</span>.<span class="number">000077</span>,delta_z=<span class="number">0</span>.<span class="number">000077</span></span><br><span class="line"><span class="attribute">factor_b</span>=<span class="number">60</span>.<span class="number">671261</span>, delta_b=<span class="number">0</span>.<span class="number">000001</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">3</span>.<span class="number">805937</span>,z=<span class="number">150</span>.<span class="number">000000</span>,delta_z=<span class="number">0</span>.<span class="number">000000</span></span><br><span class="line"><span class="attribute">done</span>!</span><br><span class="line"><span class="attribute">final</span> b=<span class="number">3</span>.<span class="number">805937</span></span><br></pre></td></tr></table></figure>
<p>从以上两个结果对比中，可以看到三点：</p>
<ol>
<li><code>factor_b</code>第一次是<code>63</code>，以后每次都会略微降低一些</li>
<li>第二个函数迭代了3次就结束了，而第一个函数迭代了5次，效率不一样</li>
<li>最后得到的结果是一样的，因为这个问题只有一个解</li>
</ol>
<p>对于双变量的迭代，有同样的问题：</p>
<p>没有在迭代中重新计算 $\Delta b,\Delta w$ 的贡献值(<code>factor_b</code>和<code>factor_w</code>每次都保持<code>63</code>和<code>18</code>)：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">double</span> variable: w, b -----</span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">4</span>.<span class="number">000000</span>,z=<span class="number">162</span>.<span class="number">000000</span>,delta_z=<span class="number">12</span>.<span class="number">000000</span></span><br><span class="line"><span class="attribute">delta_b</span>=<span class="number">0</span>.<span class="number">095238</span>, delta_w=<span class="number">0</span>.<span class="number">333333</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">2</span>.<span class="number">666667</span>,b=<span class="number">3</span>.<span class="number">904762</span>,z=<span class="number">150</span>.<span class="number">181406</span>,delta_z=<span class="number">0</span>.<span class="number">181406</span></span><br><span class="line"><span class="attribute">delta_b</span>=<span class="number">0</span>.<span class="number">001440</span>, delta_w=<span class="number">0</span>.<span class="number">005039</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">2</span>.<span class="number">661628</span>,b=<span class="number">3</span>.<span class="number">903322</span>,z=<span class="number">150</span>.<span class="number">005526</span>,delta_z=<span class="number">0</span>.<span class="number">005526</span></span><br><span class="line"><span class="attribute">delta_b</span>=<span class="number">0</span>.<span class="number">000044</span>, delta_w=<span class="number">0</span>.<span class="number">000154</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">2</span>.<span class="number">661474</span>,b=<span class="number">3</span>.<span class="number">903278</span>,z=<span class="number">150</span>.<span class="number">000170</span>,delta_z=<span class="number">0</span>.<span class="number">000170</span></span><br><span class="line"><span class="attribute">delta_b</span>=<span class="number">0</span>.<span class="number">000001</span>, delta_w=<span class="number">0</span>.<span class="number">000005</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">2</span>.<span class="number">661469</span>,b=<span class="number">3</span>.<span class="number">903277</span>,z=<span class="number">150</span>.<span class="number">000005</span>,delta_z=<span class="number">0</span>.<span class="number">000005</span></span><br><span class="line"><span class="attribute">done</span>!</span><br><span class="line"><span class="attribute">final</span> b=<span class="number">3</span>.<span class="number">903277</span></span><br><span class="line"><span class="attribute">final</span> w=<span class="number">2</span>.<span class="number">661469</span></span><br></pre></td></tr></table></figure>

<p>在每次迭代中都重新计算 $\Delta b,\Delta w$ 的贡献值(<code>factor_b</code>和<code>factor_w</code>每次都变化)：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">double</span> variable new: w, b -----</span><br><span class="line"><span class="attribute">w</span>=<span class="number">3</span>.<span class="number">000000</span>,b=<span class="number">4</span>.<span class="number">000000</span>,z=<span class="number">162</span>.<span class="number">000000</span>,delta_z=<span class="number">12</span>.<span class="number">000000</span></span><br><span class="line"><span class="attribute">factor_b</span>=<span class="number">63</span>.<span class="number">000000</span>, factor_w=<span class="number">18</span>.<span class="number">000000</span>, delta_b=<span class="number">0</span>.<span class="number">095238</span>, delta_w=<span class="number">0</span>.<span class="number">333333</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">2</span>.<span class="number">666667</span>,b=<span class="number">3</span>.<span class="number">904762</span>,z=<span class="number">150</span>.<span class="number">181406</span>,delta_z=<span class="number">0</span>.<span class="number">181406</span></span><br><span class="line"><span class="attribute">factor_b</span>=<span class="number">60</span>.<span class="number">523810</span>, factor_w=<span class="number">17</span>.<span class="number">619048</span>, delta_b=<span class="number">0</span>.<span class="number">001499</span>, delta_w=<span class="number">0</span>.<span class="number">005148</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">2</span>.<span class="number">661519</span>,b=<span class="number">3</span>.<span class="number">903263</span>,z=<span class="number">150</span>.<span class="number">000044</span>,delta_z=<span class="number">0</span>.<span class="number">000044</span></span><br><span class="line"><span class="attribute">factor_b</span>=<span class="number">60</span>.<span class="number">485234</span>, factor_w=<span class="number">17</span>.<span class="number">613053</span>, delta_b=<span class="number">0</span>.<span class="number">000000</span>, delta_w=<span class="number">0</span>.<span class="number">000001</span></span><br><span class="line"><span class="attribute">w</span>=<span class="number">2</span>.<span class="number">661517</span>,b=<span class="number">3</span>.<span class="number">903263</span>,z=<span class="number">150</span>.<span class="number">000000</span>,delta_z=<span class="number">0</span>.<span class="number">000000</span></span><br><span class="line"><span class="attribute">done</span>!</span><br><span class="line"><span class="attribute">final</span> b=<span class="number">3</span>.<span class="number">903263</span></span><br><span class="line"><span class="attribute">final</span> w=<span class="number">2</span>.<span class="number">661517</span></span><br></pre></td></tr></table></figure>
<p>这个与第一个单变量迭代不同的地方是：这个问题可以有多个解，所以两种方式都可以得到各自的正确解，但是第二种方式效率高，而且满足梯度下降的概念。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Backprop/">http://colah.github.io/posts/2015-08-Backprop/</a></p>
<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

<h2 id="2-2-非线性反向传播"><a href="#2-2-非线性反向传播" class="headerlink" title="2.2 非线性反向传播"></a>2.2 非线性反向传播</h2><h3 id="2-2-1-提出问题"><a href="#2-2-1-提出问题" class="headerlink" title="2.2.1 提出问题"></a>2.2.1 提出问题</h3><p>在上面的线性例子中，我们可以发现，误差一次性地传递给了初始值 $w$ 和 $b$，即，只经过一步，直接修改 $w$ 和 $b$ 的值，就能做到误差校正。因为从它的计算图看，无论中间计算过程有多么复杂，它都是线性的，所以可以一次传到底。缺点是这种线性的组合最多只能解决线性问题，不能解决更复杂的问题。这个我们在神经网络基本原理中已经阐述过了，需要有激活函数连接两个线性单元。</p>
<p>下面我们看一个非线性的例子，如图2-8所示。</p>
<p><img src="/../imgs/image-78.png" alt="图2-8 非线性的反向传播"></p>
<p>其中$1&lt;x&lt;&#x3D;10,0&lt;y&lt;2.15$。假设有5个人分别代表 $x,a,b,c,y$：</p>
<h4 id="正向过程"><a href="#正向过程" class="headerlink" title="正向过程"></a>正向过程</h4><ol>
<li>第1个人，输入层，随机输入第一个 $x$ 值，$x$ 的取值范围 $(1,10]$，假设第一个数是 $2$；</li>
<li>第2个人，第一层网络计算，接收第1个人传入 $x$ 的值，计算：$a&#x3D;x^2$；</li>
<li>第3个人，第二层网络计算，接收第2个人传入 $a$ 的值，计算：$b&#x3D;\ln (a)$；</li>
<li>第4个人，第三层网络计算，接收第3个人传入 $b$ 的值，计算：$c&#x3D;\sqrt{b}$；</li>
<li>第5个人，输出层，接收第4个人传入 $c$ 的值</li>
</ol>
<h4 id="反向过程"><a href="#反向过程" class="headerlink" title="反向过程"></a>反向过程</h4><ol start="6">
<li>第5个人，计算 $y$ 与 $c$ 的差值：$\Delta c &#x3D; c - y$，传回给第4个人</li>
<li>第4个人，接收第5个人传回$\Delta c$，计算 $\Delta b &#x3D; \Delta c \cdot 2\sqrt{b}$</li>
<li>第3个人，接收第4个人传回$\Delta b$，计算 $\Delta a &#x3D; \Delta b \cdot a$</li>
<li>第2个人，接收第3个人传回$\Delta a$，计算 $\Delta x &#x3D; \frac{\Delta}{2x}$</li>
<li>第1个人，接收第2个人传回$\Delta x$，更新 $x \leftarrow x - \Delta x$，回到第1步</li>
</ol>
<p>提出问题：假设我们想最后得到 $c&#x3D;2.13$ 的值，$x$ 应该是多少？（误差小于 $0.001$ 即可）</p>
<h3 id="2-2-2-数学解析解"><a href="#2-2-2-数学解析解" class="headerlink" title="2.2.2 数学解析解"></a>2.2.2 数学解析解</h3><p>$$<br>c&#x3D;\sqrt{b}&#x3D;\sqrt{\ln(a)}&#x3D;\sqrt{\ln(x^2)}&#x3D;2.13<br>$$</p>
<p>$$<br>x &#x3D; 9.6653<br>$$</p>
<h3 id="2-2-3-梯度迭代解"><a href="#2-2-3-梯度迭代解" class="headerlink" title="2.2.3 梯度迭代解"></a>2.2.3 梯度迭代解</h3><p>$$<br>\frac{da}{dx}&#x3D;\frac{d(x^2)}{dx}&#x3D;2x&#x3D;\frac{\Delta a}{\Delta x} \tag{1}<br>$$</p>
<p>$$<br>\frac{db}{da} &#x3D;\frac{d(\ln{a})}{da} &#x3D;\frac{1}{a} &#x3D; \frac{\Delta b}{\Delta a} \tag{2}<br>$$</p>
<p>$$<br>\frac{dc}{db}&#x3D;\frac{d(\sqrt{b})}{db}&#x3D;\frac{1}{2\sqrt{b}}&#x3D;\frac{\Delta c}{\Delta b} \tag{3}<br>$$</p>
<p>因此得到如下一组公式，可以把最后一层 $\Delta c$ 的误差一直反向传播给最前面的 $\Delta x$，从而更新 $x$ 值：</p>
<p>$$<br>\Delta c &#x3D; c - y \tag{4}<br>$$</p>
<p>根据式3</p>
<p>$$<br>\Delta b &#x3D; \Delta c \cdot 2\sqrt{b}<br>$$</p>
<p>根据式2</p>
<p>$$<br>\Delta a &#x3D; \Delta b \cdot a<br>$$</p>
<p>根据式1</p>
<p>$$<br>\Delta x &#x3D; \Delta a &#x2F; 2x<br>$$</p>
<p>我们给定初始值 $x&#x3D;2$，$\Delta x&#x3D;0$，依次计算结果如表2-2。</p>
<p>表2-2 正向与反向的迭代计算</p>
<table>
<thead>
<tr>
<th>方向</th>
<th>公式</th>
<th>迭代1</th>
<th>迭代2</th>
<th>迭代3</th>
<th>迭代4</th>
<th>迭代5</th>
</tr>
</thead>
<tbody><tr>
<td>正向</td>
<td>$x&#x3D;x-\Delta x$</td>
<td>2</td>
<td>4.243</td>
<td>7.344</td>
<td>9.295</td>
<td>9.665</td>
</tr>
<tr>
<td>正向</td>
<td>$a&#x3D;x^2$</td>
<td>4</td>
<td>18.005</td>
<td>53.934</td>
<td>86.404</td>
<td>93.233</td>
</tr>
<tr>
<td>正向</td>
<td>$b&#x3D;\ln(a)$</td>
<td>1.386</td>
<td>2.891</td>
<td>3.988</td>
<td>4.459</td>
<td>4.535</td>
</tr>
<tr>
<td>正向</td>
<td>$c&#x3D;\sqrt{b}$</td>
<td>1.177</td>
<td>1.700</td>
<td>1.997</td>
<td>2.112</td>
<td>2.129</td>
</tr>
<tr>
<td></td>
<td>标签值y</td>
<td>2.13</td>
<td>2.13</td>
<td>2.13</td>
<td>2.13</td>
<td>2.13</td>
</tr>
<tr>
<td>反向</td>
<td>$\Delta c &#x3D; c - y$</td>
<td>-0.953</td>
<td>-0.430</td>
<td>-0.133</td>
<td>-0.018</td>
<td></td>
</tr>
<tr>
<td>反向</td>
<td>$\Delta b &#x3D; \Delta c \cdot 2\sqrt{b}$</td>
<td>-2.243</td>
<td>-1.462</td>
<td>-0.531</td>
<td>-0.078</td>
<td></td>
</tr>
<tr>
<td>反向</td>
<td>$\Delta a &#x3D; \Delta b \cdot a$</td>
<td>-8.973</td>
<td>-26.317</td>
<td>-28.662</td>
<td>-6.698</td>
<td></td>
</tr>
<tr>
<td>反向</td>
<td>$\Delta x &#x3D; \Delta a &#x2F; 2x$</td>
<td>-2.243</td>
<td>-3.101</td>
<td>-1.951</td>
<td>-0.360</td>
<td></td>
</tr>
</tbody></table>
<p>表2-2，先看“迭代-1”列，从上到下是一个完整的正向+反向的过程，最后一行是 $-2.243$，回到“迭代-2”列的第一行，$2-(-2.243)&#x3D;4.243$，然后继续向下。到第5轮时，正向计算得到的 $c&#x3D;2.129$，非常接近 $2.13$ 了，迭代结束。</p>
<p>运行示例代码可以得到如下结果：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">how <span class="keyword">to</span> play: 1) input x, 2) calculate c, 3) input target number but <span class="keyword">not</span> faraway <span class="keyword">from</span> c</span><br><span class="line">input x as initial number(1.2,10), you can try 1.3:</span><br><span class="line">2</span><br><span class="line"><span class="attribute">c</span>=1.177410</span><br><span class="line">input y as target number(0.5,2), you can try 1.8:</span><br><span class="line">2.13</span><br><span class="line">forward<span class="built_in">..</span>.</span><br><span class="line"><span class="attribute">x</span>=2.000000,a=4.000000,b=1.386294,c=1.177410</span><br><span class="line">backward<span class="built_in">..</span>.</span><br><span class="line"><span class="attribute">delta_c</span>=-0.952590, <span class="attribute">delta_b</span>=-2.243178, <span class="attribute">delta_a</span>=-8.972712, <span class="attribute">delta_x</span>=-2.243178</span><br><span class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span></span><br><span class="line">forward<span class="built_in">..</span>.</span><br><span class="line"><span class="attribute">x</span>=9.655706,a=93.232666,b=4.535098,c=2.129577</span><br><span class="line">backward<span class="built_in">..</span>.</span><br><span class="line">done!</span><br></pre></td></tr></table></figure>

<p>为节省篇幅只列出了第一步和最后一步（第5步）的结果，第一步时<code>c=1.177410</code>，最后一步时<code>c=2.129577</code>，停止迭代。</p>
<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

<h2 id="2-3-梯度下降"><a href="#2-3-梯度下降" class="headerlink" title="2.3 梯度下降"></a>2.3 梯度下降</h2><h3 id="2-3-1-从自然现象中理解梯度下降"><a href="#2-3-1-从自然现象中理解梯度下降" class="headerlink" title="2.3.1 从自然现象中理解梯度下降"></a>2.3.1 从自然现象中理解梯度下降</h3><p>在大多数文章中，都以“一个人被困在山上，需要迅速下到谷底”来举例，这个人会“寻找当前所处位置最陡峭的地方向下走”。这个例子中忽略了安全因素，这个人不可能沿着最陡峭的方向走，要考虑坡度。</p>
<p>在自然界中，梯度下降的最好例子，就是泉水下山的过程：</p>
<ol>
<li>水受重力影响，会在当前位置，沿着最陡峭的方向流动，有时会形成瀑布（梯度下降）；</li>
<li>水流下山的路径不是唯一的，在同一个地点，有可能有多个位置具有同样的陡峭程度，而造成了分流（可以得到多个解）；</li>
<li>遇到坑洼地区，有可能形成湖泊，而终止下山过程（不能得到全局最优解，而是局部最优解）。</li>
</ol>
<h3 id="2-3-2-梯度下降的数学理解"><a href="#2-3-2-梯度下降的数学理解" class="headerlink" title="2.3.2 梯度下降的数学理解"></a>2.3.2 梯度下降的数学理解</h3><p>梯度下降的数学公式：</p>
<p>$$\theta_{n+1} &#x3D; \theta_{n} - \eta \cdot \nabla J(\theta) \tag{1}$$</p>
<p>其中：</p>
<ul>
<li>$\theta_{n+1}$：下一个值；</li>
<li>$\theta_n$：当前值；</li>
<li>$-$：减号，梯度的反向；</li>
<li>$\eta$：学习率或步长，控制每一步走的距离，不要太快以免错过了最佳景点，不要太慢以免时间太长；</li>
<li>$\nabla$：梯度，函数当前位置的最快上升点；</li>
<li>$J(\theta)$：函数。</li>
</ul>
<h4 id="梯度下降的三要素"><a href="#梯度下降的三要素" class="headerlink" title="梯度下降的三要素"></a>梯度下降的三要素</h4><ol>
<li>当前点；</li>
<li>方向；</li>
<li>步长。</li>
</ol>
<h4 id="为什么说是“梯度下降”？"><a href="#为什么说是“梯度下降”？" class="headerlink" title="为什么说是“梯度下降”？"></a>为什么说是“梯度下降”？</h4><p>“梯度下降”包含了两层含义：</p>
<ol>
<li>梯度：函数当前位置的最快上升点；</li>
<li>下降：与导数相反的方向，用数学语言描述就是那个减号。</li>
</ol>
<p>亦即与上升相反的方向运动，就是下降。</p>
<p><img src="/../imgs/image-79.png" alt="图2-9 梯度下降的步骤"></p>
<p>图2-9解释了在函数极值点的两侧做梯度下降的计算过程，梯度下降的目的就是使得x值向极值点逼近。</p>
<h3 id="2-3-3-单变量函数的梯度下降"><a href="#2-3-3-单变量函数的梯度下降" class="headerlink" title="2.3.3 单变量函数的梯度下降"></a>2.3.3 单变量函数的梯度下降</h3><p>假设一个单变量函数：</p>
<p>$$J(x) &#x3D; x ^2$$</p>
<p>我们的目的是找到该函数的最小值，于是计算其微分：</p>
<p>$$J’(x) &#x3D; 2x$$</p>
<p>假设初始位置为：</p>
<p>$$x_0&#x3D;1.2$$</p>
<p>假设学习率：</p>
<p>$$\eta &#x3D; 0.3$$</p>
<p>根据公式(1)，迭代公式：</p>
<p>$$x_{n+1} &#x3D; x_{n} - \eta \cdot \nabla J(x)&#x3D; x_{n} - \eta \cdot 2x$$</p>
<p>假设终止条件为 $J(x)&lt;0.01$，迭代过程是：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">x</span>=<span class="number">0</span>.<span class="number">480000</span>, y=<span class="number">0</span>.<span class="number">230400</span></span><br><span class="line"><span class="attribute">x</span>=<span class="number">0</span>.<span class="number">192000</span>, y=<span class="number">0</span>.<span class="number">036864</span></span><br><span class="line"><span class="attribute">x</span>=<span class="number">0</span>.<span class="number">076800</span>, y=<span class="number">0</span>.<span class="number">005898</span></span><br><span class="line"><span class="attribute">x</span>=<span class="number">0</span>.<span class="number">030720</span>, y=<span class="number">0</span>.<span class="number">000944</span></span><br></pre></td></tr></table></figure>

<p>上面的过程如图2-10所示。</p>
<p><img src="/../imgs/image-80.png" alt="图2-10 使用梯度下降法迭代的过程"></p>
<h3 id="2-3-4-双变量的梯度下降"><a href="#2-3-4-双变量的梯度下降" class="headerlink" title="2.3.4 双变量的梯度下降"></a>2.3.4 双变量的梯度下降</h3><p>假设一个双变量函数：</p>
<p>$$J(x,y) &#x3D; x^2 + \sin^2(y)$$</p>
<p>我们的目的是找到该函数的最小值，于是计算其微分：</p>
<p>$${\partial{J(x,y)} \over \partial{x}} &#x3D; 2x$$<br>$${\partial{J(x,y)} \over \partial{y}} &#x3D; 2 \sin y \cos y$$</p>
<p>假设初始位置为：</p>
<p>$$(x_0,y_0)&#x3D;(3,1)$$</p>
<p>假设学习率：</p>
<p>$$\eta &#x3D; 0.1$$</p>
<p>根据公式(1)，迭代过程是的计算公式：</p>
<p>$$<br>(x_{n+1},y_{n+1}) &#x3D; (x_n,y_n) - \eta \cdot \nabla J(x,y)<br>$$</p>
<p>$$<br> &#x3D; (x_n,y_n) - \eta \cdot (2x,2 \cdot \sin y \cdot \cos y) \tag{1}<br>$$</p>
<p>根据公式(1)，假设终止条件为 $J(x,y)&lt;0.01$，迭代过程如表2-3所示。</p>
<p>表2-3 双变量梯度下降的迭代过程</p>
<table>
<thead>
<tr>
<th>迭代次数</th>
<th>x</th>
<th>y</th>
<th>J(x,y)</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>3</td>
<td>1</td>
<td>9.708073</td>
</tr>
<tr>
<td>2</td>
<td>2.4</td>
<td>0.909070</td>
<td>6.382415</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>15</td>
<td>0.105553</td>
<td>0.063481</td>
<td>0.015166</td>
</tr>
<tr>
<td>16</td>
<td>0.084442</td>
<td>0.050819</td>
<td>0.009711</td>
</tr>
</tbody></table>
<p>迭代16次后，$J(x,y)$ 的值为 $0.009711$，满足小于 $0.01$ 的条件，停止迭代。</p>
<p>上面的过程如表2-4所示，由于是双变量，所以需要用三维图来解释。请注意看两张图中间那条隐隐的黑色线，表示梯度下降的过程，从红色的高地一直沿着坡度向下走，直到蓝色的洼地。</p>
<p>表2-4 在三维空间内的梯度下降过程</p>
<p><img src="/../imgs/image-81.png"></p>
<h3 id="2-3-5-学习率η的选择"><a href="#2-3-5-学习率η的选择" class="headerlink" title="2.3.5 学习率η的选择"></a>2.3.5 学习率η的选择</h3><p>在公式表达时，学习率被表示为$\eta$。在代码里，我们把学习率定义为<code>learning_rate</code>，或者<code>eta</code>。针对上面的例子，试验不同的学习率对迭代情况的影响，如表2-5所示。</p>
<p>表2-5 不同学习率对迭代情况的影响</p>
<p><img src="/../imgs/image-83.png"></p>
<h2 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeuralNet</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Neural Network class</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, eta</span>):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.w = <span class="number">0</span></span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        z = self.w * x + self.b</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__backward</span>(<span class="params">self, x, y, z</span>):</span><br><span class="line">        dz = z - y  <span class="comment"># dz 表示预测值 z 和真实值 y 之间的误差</span></span><br><span class="line">        db = dz <span class="comment"># 因为偏置项 b 对于每个输入样本 x 都是相同的，所以误差 dz 直接反映了偏置项 b 的更新量。因此，db = dz 表示偏置项 b 的梯度（即偏导数）等于误差 dz。</span></span><br><span class="line">        dw = x * dz <span class="comment"># 损失函数对w求梯度（偏导数），损失函数为均方误差，所以损失函数对w求导，即dw = x * dz</span></span><br><span class="line">        <span class="keyword">return</span> dw, db</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度更新</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__update</span>(<span class="params">self, dw, db</span>):</span><br><span class="line">        self.w = self.w - self.eta * dw</span><br><span class="line">        self.b = self.b - self.eta * db</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, dataReader</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(dataReader.num_train_data):</span><br><span class="line">            x, y = dataReader.get_train_data(i)  <span class="comment"># 获取第i个训练样本</span></span><br><span class="line">            z = self.__forward(x)  <span class="comment"># 前向计算得到z</span></span><br><span class="line">            dw, db = self.__backward(x, y, z) <span class="comment"># 反向传播得到dw和db</span></span><br><span class="line">            self.__update(dw, db) <span class="comment"># 更新参数w和b</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inference</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.__forward(x)</span><br></pre></td></tr></table></figure></div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://kelinkong.github.io/2024/12/19/AI-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E4%B8%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" data-id="cm4wgilek0036hoogezuvc1bb" data-qrcode="data:image/png;base64,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">Share</a><div class="tags"></div><div class="post-nav"><a class="pre" href="/2024/12/20/AI-GraphRAG/">AI-GraphRAG</a><a class="next" href="/2024/12/17/Java-%E5%BC%82%E6%AD%A5%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8/">Java-异步接口调用</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://kelinkong.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img class="nofancybox" src="/img/logo.jpg"/></a><p>Kelin is a boring girl, but not always.</p><a class="info-icon" href="https://github.com/kelinkong" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Digital-Human/">Digital Human</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Frontend/">Frontend</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Network-Security/">Network-Security</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Operating-System/">Operating-System</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/frontend/">frontend</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/solutions/">solutions</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/CMU15-445/" style="font-size: 15px;">CMU15-445</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/architecture/" style="font-size: 15px;">architecture</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/06/05/Java-Java%E7%9A%84%E5%AE%B9%E5%99%A8%E5%92%8CC-%E7%9A%84STL%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AF%B9%E6%AF%94/">Java-Java的容器和C++的STL容器的对比</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/06/05/%E6%95%B0%E5%AD%97%E4%BA%BA-Java%E9%9F%B3%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86-%E9%9F%B3%E9%A2%91%E6%8E%A8%E6%B5%81%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/">数字人-Java音视频处理-音频推流失败问题排查</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/05/21/%E6%95%B0%E5%AD%97%E4%BA%BA-Java%E9%9F%B3%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86/">数字人-Java音视频处理-推流和拉流</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/04/25/%E6%95%B0%E5%AD%97%E4%BA%BA-%E5%9F%BA%E7%A1%80%E6%89%AB%E7%9B%B2/">数字人-基础扫盲</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/04/18/Java-%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/">Java-基础学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/13/deepseek-v3%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E5%AD%A6%E4%B9%A0/">deepseek-v3技术报告学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/01/10/DB-%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93Neo4j/">DB-图数据库Neo4j</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/01/09/Java-%E7%BC%96%E8%AF%91%E7%9B%B8%E5%85%B3/">Java-编译相关</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/27/AI-%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0/">AI-动手学深度学习-笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/24/AI-%E5%9F%BA%E4%BA%8Etransformers%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-NLP-%E5%85%A5%E9%97%A8/">AI-基于transformers的自然语言处理(NLP)入门</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Kelin's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/love.js?v=1.0.0"></script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>