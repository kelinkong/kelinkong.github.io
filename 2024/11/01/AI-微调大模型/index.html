<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>AI-微调大模型 | Kelin's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">AI-微调大模型</h1><a id="logo" href="/.">Kelin's blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">AI-微调大模型</h1><div class="post-meta">2024-11-01<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.3k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 6</span><span class="post-meta-item-text"> Minutes</span></span></span></div><div class="post-content"><h2 id="微调大模型"><a href="#微调大模型" class="headerlink" title="微调大模型"></a>微调大模型</h2><p>源代码来自:<a target="_blank" rel="noopener" href="https://www.cnblogs.com/obullxl/p/18312594/NTopic2024071801">老牛同学</a></p>
<p>使用开源大模型 Qwen2-0.5B 的示例，实现了一个基于微调和 RAG（Retrieval-Augmented Generation）的文本分类助手。以下是各部分的详细解释：</p>
<h3 id="1-引入必要库"><a href="#1-引入必要库" class="headerlink" title="1. 引入必要库"></a>1. 引入必要库</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> swanlab.integration.huggingface <span class="keyword">import</span> SwanLabCallback</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, TaskType, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> swanlab</span><br></pre></td></tr></table></figure>

<p>这部分代码引入了主要用于微调、训练和生成文本的库，包括 <code>transformers</code>、<code>peft</code>（主要用于 LoRA 微调）、<code>datasets</code>（用于处理数据集），以及 <code>swanlab</code> 用于回调和日志记录。</p>
<h3 id="2-设置路径和设备"><a href="#2-设置路径和设备" class="headerlink" title="2. 设置路径和设备"></a>2. 设置路径和设备</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">BASE_DIR = <span class="string">&#x27;D:\\ModelSpace\\Qwen2&#x27;</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br></pre></td></tr></table></figure>
<p>设置模型的根目录和设备名称。设备名称判断系统是否支持 CUDA（GPU 加速），如果不支持则使用 CPU。</p>
<p>在mac中使用mps加速</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;mps&quot;</span> <span class="keyword">if</span> torch.backends.mps.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="3-数据集格式转换函数"><a href="#3-数据集格式转换函数" class="headerlink" title="3. 数据集格式转换函数"></a>3. 数据集格式转换函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_jsonl_transfer</span>(<span class="params">origin_path, new_path</span>):</span><br><span class="line">    messages = []</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(origin_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">            data = json.loads(line)</span><br><span class="line">            text = data[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">            catagory = data[<span class="string">&quot;category&quot;</span>]</span><br><span class="line">            output = data[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">            message = &#123;</span><br><span class="line">                <span class="string">&quot;input&quot;</span>: <span class="string">f&quot;文本:<span class="subst">&#123;text&#125;</span>,分类选项列表:<span class="subst">&#123;catagory&#125;</span>&quot;</span>,</span><br><span class="line">                <span class="string">&quot;output&quot;</span>: output,</span><br><span class="line">            &#125;</span><br><span class="line">            messages.append(message)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(new_path, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        <span class="keyword">for</span> message <span class="keyword">in</span> messages:</span><br><span class="line">            file.write(json.dumps(message, ensure_ascii=<span class="literal">False</span>) + <span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><code>dataset_jsonl_transfer</code> 函数用于将原始 JSON 数据转换成微调所需的数据格式。原始文件的每行包含一个 JSON 对象，函数将其读取、重构并保存成新的 JSONL 格式文件，每行包含一个示例数据。</p>
<h3 id="4-数据预处理函数"><a href="#4-数据预处理函数" class="headerlink" title="4. 数据预处理函数"></a>4. 数据预处理函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_func</span>(<span class="params">example</span>):</span><br><span class="line">    MAX_LENGTH = <span class="number">384</span></span><br><span class="line">    instruction = tokenizer(<span class="string">f&quot;&lt;|im_start|&gt;system\n你是一个文本分类领域的专家...<span class="subst">&#123;example[<span class="string">&#x27;input&#x27;</span>]&#125;</span>&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;</span>, add_special_tokens=<span class="literal">False</span>)</span><br><span class="line">    response = tokenizer(<span class="string">f&quot;<span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span>, add_special_tokens=<span class="literal">False</span>)</span><br><span class="line">    input_ids = instruction[<span class="string">&quot;input_ids&quot;</span>] + response[<span class="string">&quot;input_ids&quot;</span>] + [tokenizer.pad_token_id]</span><br><span class="line">    attention_mask = instruction[<span class="string">&quot;attention_mask&quot;</span>] + response[<span class="string">&quot;attention_mask&quot;</span>] + [<span class="number">1</span>]</span><br><span class="line">    labels = [-<span class="number">100</span>] * <span class="built_in">len</span>(instruction[<span class="string">&quot;input_ids&quot;</span>]) + response[<span class="string">&quot;input_ids&quot;</span>] + [tokenizer.pad_token_id]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(input_ids) &gt; MAX_LENGTH:</span><br><span class="line">        input_ids, attention_mask, labels = input_ids[:MAX_LENGTH], attention_mask[:MAX_LENGTH], labels[:MAX_LENGTH]</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: input_ids, <span class="string">&quot;attention_mask&quot;</span>: attention_mask, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br></pre></td></tr></table></figure>
<p><code>process_func</code> 函数将数据处理成大模型可以接受的格式，包含 <code>input_ids</code>、<code>attention_mask</code> 和 <code>labels</code>。这里模拟了一个对话输入，用户提问，助手返回分类输出。如果序列长度超出最大限制 <code>MAX_LENGTH</code>，进行截断。</p>
<h3 id="5-加载模型和分词器"><a href="#5-加载模型和分词器" class="headerlink" title="5. 加载模型和分词器"></a>5. 加载模型和分词器</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_dir = os.path.join(BASE_DIR, <span class="string">&#x27;Qwen2-0.5B&#x27;</span>)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=<span class="literal">False</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=device, torch_dtype=torch.bfloat16)</span><br><span class="line">model.enable_input_require_grads()</span><br></pre></td></tr></table></figure>
<p>加载模型和分词器，将 <code>bfloat16</code> 用于精度，以减少 GPU 占用量。<code>model.enable_input_require_grads()</code> 开启梯度检查点支持，以节省内存。</p>
<h3 id="6-加载和处理数据集"><a href="#6-加载和处理数据集" class="headerlink" title="6. 加载和处理数据集"></a>6. 加载和处理数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_jsonl_new_path = os.path.join(BASE_DIR, <span class="string">&#x27;train.jsonl&#x27;</span>)</span><br><span class="line">test_jsonl_new_path = os.path.join(BASE_DIR, <span class="string">&#x27;test.jsonl&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(train_jsonl_new_path):</span><br><span class="line">    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(test_jsonl_new_path):</span><br><span class="line">    dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)</span><br><span class="line"></span><br><span class="line">train_df = pd.read_json(train_jsonl_new_path, lines=<span class="literal">True</span>)</span><br><span class="line">train_ds = Dataset.from_pandas(train_df)</span><br><span class="line">train_dataset = train_ds.<span class="built_in">map</span>(process_func, remove_columns=train_ds.column_names)</span><br></pre></td></tr></table></figure>
<p>检查并转换数据集，将其加载为 <code>Dataset</code> 格式，并通过 <code>process_func</code> 处理成可用于训练的数据格式。</p>
<h3 id="7-LoRA-配置与应用"><a href="#7-LoRA-配置与应用" class="headerlink" title="7. LoRA 配置与应用"></a>7. LoRA 配置与应用</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">config = LoraConfig(</span><br><span class="line">    task_type=TaskType.CAUSAL_LM,</span><br><span class="line">    target_modules=[<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>, ...],</span><br><span class="line">    inference_mode=<span class="literal">False</span>,</span><br><span class="line">    r=<span class="number">8</span>,</span><br><span class="line">    lora_alpha=<span class="number">32</span>,</span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = get_peft_model(model, config)</span><br></pre></td></tr></table></figure>
<p>设置并应用 LoRA（Low-Rank Adaptation）配置，用于高效微调。<code>LoRA</code> 能通过添加低秩矩阵在不改变原模型参数的情况下更新模型，适合大模型的微调。</p>
<h3 id="8-训练参数与-Trainer-初始化"><a href="#8-训练参数与-Trainer-初始化" class="headerlink" title="8. 训练参数与 Trainer 初始化"></a>8. 训练参数与 Trainer 初始化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">args = TrainingArguments(</span><br><span class="line">    output_dir=os.path.join(BASE_DIR, <span class="string">&#x27;output&#x27;</span>, <span class="string">&#x27;Qwen2-0.5B&#x27;</span>),</span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">    gradient_accumulation_steps=<span class="number">4</span>,</span><br><span class="line">    logging_steps=<span class="number">10</span>,</span><br><span class="line">    num_train_epochs=<span class="number">2</span>,</span><br><span class="line">    save_steps=<span class="number">100</span>,</span><br><span class="line">    learning_rate=<span class="number">1e-4</span>,</span><br><span class="line">    save_on_each_node=<span class="literal">True</span>,</span><br><span class="line">    gradient_checkpointing=<span class="literal">True</span>,</span><br><span class="line">    report_to=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">swanlab_callback = SwanLabCallback(project=<span class="string">&quot;Qwen2-FineTuning&quot;</span>, experiment_name=<span class="string">&quot;Qwen2-0.5B&quot;</span>)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=<span class="literal">True</span>),</span><br><span class="line">    callbacks=[swanlab_callback],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>配置训练参数并创建 <code>Trainer</code> 实例，指定保存路径、batch 大小、梯度累积步数、日志记录频率、学习率等。<code>SwanLabCallback</code> 用于将训练过程发送至 <code>SwanLab</code> 进行实时监控。</p>
<h3 id="9-训练模型"><a href="#9-训练模型" class="headerlink" title="9. 训练模型"></a>9. 训练模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<p>调用 <code>.train()</code> 开始训练。</p>
<h3 id="10-模型推理与评估"><a href="#10-模型推理与评估" class="headerlink" title="10. 模型推理与评估"></a>10. 模型推理与评估</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">messages, model, tokenizer</span>):</span><br><span class="line">    text = tokenizer.apply_chat_template(messages, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br><span class="line">    model_inputs = tokenizer([text], return_tensors=<span class="string">&quot;pt&quot;</span>).to(device)</span><br><span class="line">    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=<span class="number">512</span>)</span><br><span class="line">    generated_ids = [output_ids[<span class="built_in">len</span>(input_ids):] <span class="keyword">for</span> input_ids, output_ids <span class="keyword">in</span> <span class="built_in">zip</span>(model_inputs.input_ids, generated_ids)]</span><br><span class="line">    <span class="keyword">return</span> tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><code>predict</code> 函数实现了模型推理功能，将输入转成模型格式后生成输出文本。</p>
<h3 id="11-测试集上的推理"><a href="#11-测试集上的推理" class="headerlink" title="11. 测试集上的推理"></a>11. 测试集上的推理</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_df = pd.read_json(test_jsonl_new_path, lines=<span class="literal">True</span>)[:<span class="number">10</span>]</span><br><span class="line">test_text_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> test_df.iterrows():</span><br><span class="line">    instruction = row[<span class="string">&#x27;你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项列表，请输出文本内容的正确分类&#x27;</span>]</span><br><span class="line">    input_value = row[<span class="string">&#x27;input&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;<span class="subst">&#123;instruction&#125;</span>&quot;</span>&#125;, &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;<span class="subst">&#123;input_value&#125;</span>&quot;</span>&#125;]</span><br><span class="line">    response = predict(messages, model, tokenizer)</span><br><span class="line">    messages.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;<span class="subst">&#123;response&#125;</span>&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line">    result_text = <span class="string">f&quot;<span class="subst">&#123;messages[<span class="number">0</span>]&#125;</span>\n\n<span class="subst">&#123;messages[<span class="number">1</span>]&#125;</span>\n\n<span class="subst">&#123;messages[<span class="number">2</span>]&#125;</span>&quot;</span></span><br><span class="line">    test_text_list.append(swanlab.Text(result_text, caption=response))</span><br><span class="line"></span><br><span class="line">swanlab.log(&#123;<span class="string">&quot;Prediction&quot;</span>: test_text_list&#125;)</span><br><span class="line">swanlab.finish()</span><br></pre></td></tr></table></figure>
<p>在测试集上对模型进行评估，将预测结果和输入对比，并将输出文本记录到 <code>SwanLab</code>。</p>
</div><div class="tags"></div><div class="post-nav"><a class="pre" href="/2024/11/06/AI-%E4%BD%BF%E7%94%A8LCEL-%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84-LLM-%E5%BA%94%E7%94%A8/">AI-使用 LCEL 构建一个简单的 LLM 应用</a><a class="next" href="/2024/10/29/AI-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">AI-机器学习基础</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://kelinkong.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img class="nofancybox" src="/img/logo.jpg"/></a><p>Kelin is a boring girl, but not always.</p><a class="info-icon" href="https://github.com/kelinkong" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/CPP/">CPP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Frontend/">Frontend</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LLVM/">LLVM</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Network-Security/">Network Security</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Operating-System/">Operating System</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Solutions/">Solutions</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/CMU15-445/" style="font-size: 15px;">CMU15-445</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/CPP/" style="font-size: 15px;">CPP</a> <a href="/tags/note/" style="font-size: 15px;">note</a> <a href="/tags/architecture/" style="font-size: 15px;">architecture</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/12/20/AI-GraphRAG/">AI-GraphRAG</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/19/AI-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E4%B8%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">AI-神经网络中的三个基本概念：梯度下降、反向传播、损失函数</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/12/17/Java-%E5%BC%82%E6%AD%A5%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8/">Java-异步接口调用</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/22/%E5%89%8D%E7%AB%AF-expo/">前端-expo</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/19/Spring-Cloud-%E6%96%AD%E8%B7%AF%E5%99%A8/">Spring Cloud: 断路器</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/14/Spring-Cloud-OpenFeign/">Spring Cloud: OpenFeign</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/14/Spring-Cloud-%E6%96%B0%E5%A2%9E%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%9D%97/">Spring Cloud:服务注册与发现</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/13/Spring-Cloud%EF%BC%9Abase%E5%B7%A5%E7%A8%8B/">Spring Cloud：base工程</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/13/Java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/">Java学习笔记-配置文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/11/07/AI-%E4%BD%BF%E7%94%A8langchain-chatchat%E5%92%8Collama%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B/">llvm-使用langchain-chatchat和ollama构建大模型</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Kelin's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/love.js?v=1.0.0"></script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>