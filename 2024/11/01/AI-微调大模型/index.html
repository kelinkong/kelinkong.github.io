<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>AI-微调大模型 | Kelin's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">AI-微调大模型</h1><a id="logo" href="/.">Kelin's blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">AI-微调大模型</h1><div class="post-meta">2024-11-01<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.3k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 6</span><span class="post-meta-item-text"> Minutes</span></span></span></div><div class="post-content"><h2 id="微调大模型">微调大模型</h2>
<p>源代码来自:<a target="_blank" rel="noopener" href="https://www.cnblogs.com/obullxl/p/18312594/NTopic2024071801">老牛同学</a></p>
<p>使用开源大模型 Qwen2-0.5B 的示例，实现了一个基于微调和 RAG（Retrieval-Augmented Generation）的文本分类助手。以下是各部分的详细解释：</p>
<h3 id="1-引入必要库">1. 引入必要库</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> swanlab.integration.huggingface <span class="keyword">import</span> SwanLabCallback</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, TaskType, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> swanlab</span><br></pre></td></tr></table></figure>
<p>这部分代码引入了主要用于微调、训练和生成文本的库，包括 <code>transformers</code>、<code>peft</code>（主要用于 LoRA 微调）、<code>datasets</code>（用于处理数据集），以及 <code>swanlab</code> 用于回调和日志记录。</p>
<h3 id="2-设置路径和设备">2. 设置路径和设备</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BASE_DIR = <span class="string">&#x27;D:\\ModelSpace\\Qwen2&#x27;</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br></pre></td></tr></table></figure>
<p>设置模型的根目录和设备名称。设备名称判断系统是否支持 CUDA（GPU 加速），如果不支持则使用 CPU。</p>
<p>在mac中使用mps加速</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;mps&quot;</span> <span class="keyword">if</span> torch.backends.mps.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-数据集格式转换函数">3. 数据集格式转换函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dataset_jsonl_transfer</span>(<span class="params">origin_path, new_path</span>):</span><br><span class="line">    messages = []</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(origin_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">            data = json.loads(line)</span><br><span class="line">            text = data[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">            catagory = data[<span class="string">&quot;category&quot;</span>]</span><br><span class="line">            output = data[<span class="string">&quot;output&quot;</span>]</span><br><span class="line">            message = &#123;</span><br><span class="line">                <span class="string">&quot;input&quot;</span>: <span class="string">f&quot;文本:<span class="subst">&#123;text&#125;</span>,分类选项列表:<span class="subst">&#123;catagory&#125;</span>&quot;</span>,</span><br><span class="line">                <span class="string">&quot;output&quot;</span>: output,</span><br><span class="line">            &#125;</span><br><span class="line">            messages.append(message)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(new_path, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        <span class="keyword">for</span> message <span class="keyword">in</span> messages:</span><br><span class="line">            file.write(json.dumps(message, ensure_ascii=<span class="literal">False</span>) + <span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><code>dataset_jsonl_transfer</code> 函数用于将原始 JSON 数据转换成微调所需的数据格式。原始文件的每行包含一个 JSON 对象，函数将其读取、重构并保存成新的 JSONL 格式文件，每行包含一个示例数据。</p>
<h3 id="4-数据预处理函数">4. 数据预处理函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_func</span>(<span class="params">example</span>):</span><br><span class="line">    MAX_LENGTH = <span class="number">384</span></span><br><span class="line">    instruction = tokenizer(<span class="string">f&quot;&lt;|im_start|&gt;system\n你是一个文本分类领域的专家...<span class="subst">&#123;example[<span class="string">&#x27;input&#x27;</span>]&#125;</span>&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;</span>, add_special_tokens=<span class="literal">False</span>)</span><br><span class="line">    response = tokenizer(<span class="string">f&quot;<span class="subst">&#123;example[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span>, add_special_tokens=<span class="literal">False</span>)</span><br><span class="line">    input_ids = instruction[<span class="string">&quot;input_ids&quot;</span>] + response[<span class="string">&quot;input_ids&quot;</span>] + [tokenizer.pad_token_id]</span><br><span class="line">    attention_mask = instruction[<span class="string">&quot;attention_mask&quot;</span>] + response[<span class="string">&quot;attention_mask&quot;</span>] + [<span class="number">1</span>]</span><br><span class="line">    labels = [-<span class="number">100</span>] * <span class="built_in">len</span>(instruction[<span class="string">&quot;input_ids&quot;</span>]) + response[<span class="string">&quot;input_ids&quot;</span>] + [tokenizer.pad_token_id]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(input_ids) &gt; MAX_LENGTH:</span><br><span class="line">        input_ids, attention_mask, labels = input_ids[:MAX_LENGTH], attention_mask[:MAX_LENGTH], labels[:MAX_LENGTH]</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;input_ids&quot;</span>: input_ids, <span class="string">&quot;attention_mask&quot;</span>: attention_mask, <span class="string">&quot;labels&quot;</span>: labels&#125;</span><br></pre></td></tr></table></figure>
<p><code>process_func</code> 函数将数据处理成大模型可以接受的格式，包含 <code>input_ids</code>、<code>attention_mask</code> 和 <code>labels</code>。这里模拟了一个对话输入，用户提问，助手返回分类输出。如果序列长度超出最大限制 <code>MAX_LENGTH</code>，进行截断。</p>
<h3 id="5-加载模型和分词器">5. 加载模型和分词器</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model_dir = os.path.join(BASE_DIR, <span class="string">&#x27;Qwen2-0.5B&#x27;</span>)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=<span class="literal">False</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=device, torch_dtype=torch.bfloat16)</span><br><span class="line">model.enable_input_require_grads()</span><br></pre></td></tr></table></figure>
<p>加载模型和分词器，将 <code>bfloat16</code> 用于精度，以减少 GPU 占用量。<code>model.enable_input_require_grads()</code> 开启梯度检查点支持，以节省内存。</p>
<h3 id="6-加载和处理数据集">6. 加载和处理数据集</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train_jsonl_new_path = os.path.join(BASE_DIR, <span class="string">&#x27;train.jsonl&#x27;</span>)</span><br><span class="line">test_jsonl_new_path = os.path.join(BASE_DIR, <span class="string">&#x27;test.jsonl&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(train_jsonl_new_path):</span><br><span class="line">    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(test_jsonl_new_path):</span><br><span class="line">    dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)</span><br><span class="line"></span><br><span class="line">train_df = pd.read_json(train_jsonl_new_path, lines=<span class="literal">True</span>)</span><br><span class="line">train_ds = Dataset.from_pandas(train_df)</span><br><span class="line">train_dataset = train_ds.<span class="built_in">map</span>(process_func, remove_columns=train_ds.column_names)</span><br></pre></td></tr></table></figure>
<p>检查并转换数据集，将其加载为 <code>Dataset</code> 格式，并通过 <code>process_func</code> 处理成可用于训练的数据格式。</p>
<h3 id="7-LoRA-配置与应用">7. LoRA 配置与应用</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">config = LoraConfig(</span><br><span class="line">    task_type=TaskType.CAUSAL_LM,</span><br><span class="line">    target_modules=[<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>, ...],</span><br><span class="line">    inference_mode=<span class="literal">False</span>,</span><br><span class="line">    r=<span class="number">8</span>,</span><br><span class="line">    lora_alpha=<span class="number">32</span>,</span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = get_peft_model(model, config)</span><br></pre></td></tr></table></figure>
<p>设置并应用 LoRA（Low-Rank Adaptation）配置，用于高效微调。<code>LoRA</code> 能通过添加低秩矩阵在不改变原模型参数的情况下更新模型，适合大模型的微调。</p>
<h3 id="8-训练参数与-Trainer-初始化">8. 训练参数与 Trainer 初始化</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">args = TrainingArguments(</span><br><span class="line">    output_dir=os.path.join(BASE_DIR, <span class="string">&#x27;output&#x27;</span>, <span class="string">&#x27;Qwen2-0.5B&#x27;</span>),</span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">    gradient_accumulation_steps=<span class="number">4</span>,</span><br><span class="line">    logging_steps=<span class="number">10</span>,</span><br><span class="line">    num_train_epochs=<span class="number">2</span>,</span><br><span class="line">    save_steps=<span class="number">100</span>,</span><br><span class="line">    learning_rate=<span class="number">1e-4</span>,</span><br><span class="line">    save_on_each_node=<span class="literal">True</span>,</span><br><span class="line">    gradient_checkpointing=<span class="literal">True</span>,</span><br><span class="line">    report_to=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">swanlab_callback = SwanLabCallback(project=<span class="string">&quot;Qwen2-FineTuning&quot;</span>, experiment_name=<span class="string">&quot;Qwen2-0.5B&quot;</span>)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=<span class="literal">True</span>),</span><br><span class="line">    callbacks=[swanlab_callback],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>配置训练参数并创建 <code>Trainer</code> 实例，指定保存路径、batch 大小、梯度累积步数、日志记录频率、学习率等。<code>SwanLabCallback</code> 用于将训练过程发送至 <code>SwanLab</code> 进行实时监控。</p>
<h3 id="9-训练模型">9. 训练模型</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<p>调用 <code>.train()</code> 开始训练。</p>
<h3 id="10-模型推理与评估">10. 模型推理与评估</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">messages, model, tokenizer</span>):</span><br><span class="line">    text = tokenizer.apply_chat_template(messages, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br><span class="line">    model_inputs = tokenizer([text], return_tensors=<span class="string">&quot;pt&quot;</span>).to(device)</span><br><span class="line">    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=<span class="number">512</span>)</span><br><span class="line">    generated_ids = [output_ids[<span class="built_in">len</span>(input_ids):] <span class="keyword">for</span> input_ids, output_ids <span class="keyword">in</span> <span class="built_in">zip</span>(model_inputs.input_ids, generated_ids)]</span><br><span class="line">    <span class="keyword">return</span> tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><code>predict</code> 函数实现了模型推理功能，将输入转成模型格式后生成输出文本。</p>
<h3 id="11-测试集上的推理">11. 测试集上的推理</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">test_df = pd.read_json(test_jsonl_new_path, lines=<span class="literal">True</span>)[:<span class="number">10</span>]</span><br><span class="line">test_text_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> test_df.iterrows():</span><br><span class="line">    instruction = row[<span class="string">&#x27;你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项列表，请输出文本内容的正确分类&#x27;</span>]</span><br><span class="line">    input_value = row[<span class="string">&#x27;input&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;<span class="subst">&#123;instruction&#125;</span>&quot;</span>&#125;, &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;<span class="subst">&#123;input_value&#125;</span>&quot;</span>&#125;]</span><br><span class="line">    response = predict(messages, model, tokenizer)</span><br><span class="line">    messages.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;<span class="subst">&#123;response&#125;</span>&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line">    result_text = <span class="string">f&quot;<span class="subst">&#123;messages[<span class="number">0</span>]&#125;</span>\n\n<span class="subst">&#123;messages[<span class="number">1</span>]&#125;</span>\n\n<span class="subst">&#123;messages[<span class="number">2</span>]&#125;</span>&quot;</span></span><br><span class="line">    test_text_list.append(swanlab.Text(result_text, caption=response))</span><br><span class="line"></span><br><span class="line">swanlab.log(&#123;<span class="string">&quot;Prediction&quot;</span>: test_text_list&#125;)</span><br><span class="line">swanlab.finish()</span><br></pre></td></tr></table></figure>
<p>在测试集上对模型进行评估，将预测结果和输入对比，并将输出文本记录到 <code>SwanLab</code>。</p>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://kelinkong.github.io/2024/11/01/AI-%E5%BE%AE%E8%B0%83%E5%A4%A7%E6%A8%A1%E5%9E%8B/" data-id="cmkui6tn7000jztog9k0b0xdp" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADJUlEQVR42u3awW7cMAwE0Pz/T2+Bngq0686QXiBWnk5B4nX0dAjDob6+4vX6va5/+uf6+1PJT/P3559dLWxsbOyHsF+X692rr5+/PrLkO8mT7/YZ7Q0bGxv7OPb1r5yVoutD3OCTff7nDdjY2Ng/kp2XtPyA7jpEbGxsbOy2gF2Xok8Us3af2NjY2D+NnYRKSYTUbisvV7Mid0OWho2Njf3t2Zutf7evPzjfxsbGxv6W7NcH1nXj8YmRbb1DbGxs7IPYSTB0/fd/Vlra60H78XPEwMbGxn4gOy8YednIg6T9lZ1ZCcTGxsY+j70vPzkpKZZ3NRtvjxUbGxv7IHb+r39e0toGpn3z7Mi+8qqIjY2N/XB2W37qsrEImDZx1T+ex8bGxj6C3Y5s8yPIQ6XNMLhN/rGxsbHPY+dlJm8MkiLUHmLLLkbX2NjY2A9n39UAzK743FUy24tE2NjY2Oex7w3ik1Zk/85ZVcLGxsY+lb0J9+/6D78tjaurQtjY2NjHsTfhUdt4tGPatsm57Y4SNjY29qPYsw9vovx7Y6O6BcLGxsY+iN3ii7nxKMNpY6zNgWJjY2Ofx74uCe3aHMH1cc9GCNjY2Nhns2cj3oSdP5PvITkCbGxs7J/DnrUHbYnaR1GzIKloQrCxsbEfzm4biQQwDHqCUUEbLf1jz9jY2NgHsfMCMxu+FkVlNFqePYONjY19Ensf9Lfl6q4rPvkzb/MzbGxs7OPYeXGaBUntxZ3ZRZ+oVmNjY2MfxM7joZx0b280G/0OZyPY2NjYD2e3V2pmmHYwkA992/KMjY2NfRJ7NpptX9oeX370s+9jY2Njn82eRTz74D4/6PZQhkkbNjY29gPZyes2kdOmhcgvX9ZtEjY2NvZj2a9y5cPd2TM5bHVJFBsbG/sgdpu07AOg9gpO29IkzQw2Njb2eexNcNMGRpvRbM6LihY2Njb2cexZFpW3Lm0gtZpsBIeIjY2Njb2JjTaj3HYIjY2NjY396dFsO4poA6y3e8DGxsY+jp2UhFlpyUvOftiQt0DY2NjYJ7FnUc4sHkram3ZkuxljYGNjYz+c/QvbXL+DfZmHlwAAAABJRU5ErkJggg==">Share</a><div class="tags"></div><div class="post-nav"><a class="pre" href="/2024/11/06/AI-%E4%BD%BF%E7%94%A8LCEL-%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84-LLM-%E5%BA%94%E7%94%A8/">AI-使用 LCEL 构建一个简单的 LLM 应用</a><a class="next" href="/2024/10/29/AI-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">AI-机器学习基础</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://kelinkong.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img class="nofancybox" src="/img/logo.jpg"/></a><p>Kelin is a boring girl, but not always.</p><a class="info-icon" href="https://github.com/kelinkong" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI/">AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Digital-Human/">Digital Human</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Frontend/">Frontend</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Network-Security/">Network-Security</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Operating-System/">Operating-System</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Radar-Principle/">Radar Principle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/solutions/">solutions</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/architecture/" style="font-size: 15px;">architecture</a> <a href="/tags/CMU15-445/" style="font-size: 15px;">CMU15-445</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/%E9%9B%B7%E8%BE%BE%E5%8E%9F%E7%90%86/" style="font-size: 15px;">雷达原理</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2026/01/22/%E3%80%90c++%E3%80%91brtc%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E8%AE%B0%E5%BD%95/">【c++】brtc项目开发记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/12/08/Java-copy%E6%A8%A1%E5%BC%8F/">Java-copy模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/11/28/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8-tryHackMe%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">网络安全-学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/11/18/Java-%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F/">Java-命令模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/09/11/Java-%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E8%AE%B0%E5%BD%95/">Java-项目开发记录</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/06/13/%E9%9B%B7%E8%BE%BE%E5%8E%9F%E7%90%86-ukf-%E6%BB%A4%E6%B3%A2%E5%99%A8/">雷达原理-ukf 滤波器</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/06/13/%E9%9B%B7%E8%BE%BE%E5%8E%9F%E7%90%86-%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/">雷达原理-卡尔曼滤波器基础学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/06/05/Java-Java%E7%9A%84%E5%AE%B9%E5%99%A8%E5%92%8CC-%E7%9A%84STL%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AF%B9%E6%AF%94/">Java-Java的容器和C++的STL容器的对比</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/06/05/%E6%95%B0%E5%AD%97%E4%BA%BA-Java%E9%9F%B3%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86-%E9%9F%B3%E9%A2%91%E6%8E%A8%E6%B5%81%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/">数字人-Java音视频处理-音频推流失败问题排查</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/05/21/%E6%95%B0%E5%AD%97%E4%BA%BA-Java%E9%9F%B3%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86/">数字人-Java音视频处理-推流和拉流</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2026 <a href="/." rel="nofollow">Kelin's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/love.js?v=1.0.0"></script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>